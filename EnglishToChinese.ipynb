{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Preprocessing\" data-toc-modified-id=\"Preprocessing-1\">Preprocessing</a></span></li><li><span><a href=\"#Model\" data-toc-modified-id=\"Model-2\">Model</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import spacy\n",
    "\n",
    "#Embeddings\n",
    "glove = torchtext.vocab.GloVe(name='6B', dim=100)\n",
    "\n",
    "#tokenizer model\n",
    "nlp_en = spacy.load('en300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "wv_from_bin = KeyedVectors.load_word2vec_format(\"data/model.bin\", binary=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import jieba\n",
    "import gensim \n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "stop_words = [line.rstrip() for line in open('data/chinese_stop_words.txt',\"r\", encoding=\"utf-8\") ]\n",
    "\n",
    "def get_sentence_vector_zh(line, mean):\n",
    "    vectors = []\n",
    "    for w in line:\n",
    "        try:\n",
    "            emb = wv_from_bin[w]\n",
    "            vectors.append(emb)\n",
    "        except:\n",
    "            pass #Do not add if the word is out of vocabulary\n",
    "    if vectors:\n",
    "        vectors = np.array(vectors)\n",
    "        if mean:\n",
    "            vectors = np.mean(vectors) \n",
    "        else :\n",
    "            vectors = np.mean(vectors, axis=0)\n",
    "        return vectors\n",
    "    else:\n",
    "        return np.zeros(100)\n",
    "\n",
    "\n",
    "def processing_zh(sentence):\n",
    "    seg_list = jieba.lcut(sentence,cut_all=True)\n",
    "    doc = [word for word in seg_list if word not in stop_words]\n",
    "    docs = [e for e in doc if e.isalnum()]\n",
    "    return docs\n",
    "\n",
    "\n",
    "def get_sentence_embeddings_zh(f, mean=True):\n",
    "    file = open(f, encoding=\"utf8\") \n",
    "    lines = file.readlines() \n",
    "    sentences_vectors = []\n",
    "    for l in lines:\n",
    "        sent  = processing_zh(l)\n",
    "        vec = get_sentence_vector_zh(sent, mean=mean)\n",
    "        if vec is not None:\n",
    "            sentences_vectors.append(vec)\n",
    "        else:\n",
    "            print(l)\n",
    "    return sentences_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm \n",
    "\n",
    "stop_words_en = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(sentence,nlp):\n",
    "    text = sentence.lower()\n",
    "    doc = [token.lemma_ for token in  nlp.tokenizer(text)]\n",
    "    doc = [word for word in doc if word not in stop_words_en]\n",
    "    doc = [word for word in doc if word.isalpha()] #restricts string to alphabetic characters only\n",
    "    return doc\n",
    "\n",
    "def get_word_vector(embeddings, word):\n",
    "    try:\n",
    "        vec = embeddings.vectors[embeddings.stoi[word]]\n",
    "        return vec\n",
    "    except KeyError:\n",
    "        #print(f\"Word {word} does not exist\")\n",
    "        pass\n",
    "\n",
    "def get_sentence_vector(embeddings,line):\n",
    "    vectors = []\n",
    "    for w in line:\n",
    "        emb = get_word_vector(embeddings,w)\n",
    "        #do not add if the word is out of vocabulary\n",
    "        if emb is not None:\n",
    "            vectors.append(emb)\n",
    "    return torch.mean(torch.stack(vectors), axis=0).data.numpy()\n",
    "\n",
    "\n",
    "def get_embeddings(f,embeddings,lang):\n",
    "    file = open(f, encoding=\"utf8\") \n",
    "    lines = file.readlines() \n",
    "    sentences_vectors =[]\n",
    "\n",
    "    for l in lines:\n",
    "        sentence = preprocess(l,lang)\n",
    "        try:\n",
    "            vec = get_sentence_vector(embeddings,sentence)\n",
    "            sentences_vectors.append(vec)\n",
    "        except:\n",
    "            sentences_vectors.append(np.zeros(100))\n",
    "\n",
    "    return sentences_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "\n",
    "zh_train_mt = get_sentence_embeddings_zh(\"data/en-zh/train.enzh.mt\", mean=False)\n",
    "zh_train_src = get_embeddings(\"data/en-zh/train.enzh.src\", glove, nlp_en)\n",
    "f_train_scores = open(\"data/en-zh/train.enzh.scores\",'r')\n",
    "zh_train_scores = f_train_scores.readlines()\n",
    "\n",
    "zh_val_src = get_embeddings(\"data/en-zh/dev.enzh.src\", glove, nlp_en)\n",
    "zh_val_mt = get_sentence_embeddings_zh(\"data/en-zh/dev.enzh.mt\", mean=False)\n",
    "f_val_scores = open(\"data/en-zh/dev.enzh.scores\",'r')\n",
    "zh_val_scores = f_val_scores.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training mt: {len(zh_train_mt)} Training src: {len(zh_train_src)}\")\n",
    "print(f\"Validation mt: {len(zh_val_mt)} Validation src: {len(zh_val_src)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_train = np.concatenate((np.asarray(zh_train_src), np.asarray(zh_train_mt)),axis=1) #[np.array(zh_train_src),np.array(zh_train_mt)]\n",
    "X_train_zh = np.array(X_train).transpose()\n",
    "\n",
    "X_val = np.concatenate((zh_val_src, zh_val_mt),axis=1) # [np.array(zh_val_src),np.array(zh_val_mt)]\n",
    "X_val_zh = np.array(X_val).transpose()\n",
    "\n",
    "#Scores\n",
    "train_scores = np.array(zh_train_scores).astype(float)\n",
    "y_train_zh = train_scores\n",
    "\n",
    "val_scores = np.array(zh_val_scores).astype(float)\n",
    "y_val_zh = val_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(predictions, targets):\n",
    "    res = np.sqrt(((predictions - targets) ** 2).mean())\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(n_feature, 256)  \n",
    "        self.hidden2 = torch.nn.Linear(256, 64)\n",
    "        self.hidden3 = torch.nn.Linear(64, 16)\n",
    "        self.predict = torch.nn.Linear(16, n_output) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden(x))  \n",
    "        x = F.relu(self.hidden2(x))  \n",
    "        x = F.relu(self.hidden3(x))  \n",
    "        x = self.predict(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "net = Net(n_feature=200, n_output=1)     # define the network\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "loss_func = torch.nn.MSELoss()  \n",
    "batch_size = 64\n",
    "steps = 20000\n",
    "losses = []\n",
    "\n",
    "for t in range(steps):\n",
    "    curr_bat = np.random.choice(len(X_train), batch_size, replace=False)\n",
    "    x = torch.Tensor(X_train[curr_bat])\n",
    "    y = torch.Tensor(y_train_zh[curr_bat]).view(batch_size,-1)\n",
    "    optimizer.zero_grad()\n",
    "    prediction = net(x)    \n",
    "    #print(\"prediction.shape\", prediction, \"y.shape\", y)\n",
    "    loss = loss_func(prediction, y)\n",
    "    losses += [loss.item()]\n",
    "    optimizer.zero_grad()   \n",
    "    loss.backward()        \n",
    "    optimizer.step()    \n",
    "    if t % (steps // 10) == 0:\n",
    "        with torch.no_grad():\n",
    "            predictions = net(torch.Tensor(X_val)).flatten().data.numpy()\n",
    "            pearson = pearsonr(y_val_zh, predictions)\n",
    "            print(f'Steps: {t} RMSE: {rmse(predictions,y_val_zh)} Pearson {pearson[0]}')\n",
    "plt.plot(list(range(len(losses))), losses)\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training\")\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
