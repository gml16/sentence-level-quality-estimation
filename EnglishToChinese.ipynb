{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Preprocessing\" data-toc-modified-id=\"Preprocessing-1\">Preprocessing</a></span></li><li><span><a href=\"#Models\" data-toc-modified-id=\"Models-2\">Models</a></span><ul class=\"toc-item\"><li><span><a href=\"#SVM\" data-toc-modified-id=\"SVM-2.1\">SVM</a></span></li><li><span><a href=\"#MLP\" data-toc-modified-id=\"MLP-2.2\">MLP</a></span></li><li><span><a href=\"#RNN\" data-toc-modified-id=\"RNN-2.3\">RNN</a></span><ul class=\"toc-item\"><li><span><a href=\"#GRU\" data-toc-modified-id=\"GRU-2.3.1\">GRU</a></span></li></ul></li></ul></li><li><span><a href=\"#Save-results\" data-toc-modified-id=\"Save-results-3\">Save results</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-02-28 13:10:50--  https://github.com/Tony607/Chinese_sentiment_analysis/blob/master/data/chinese_stop_words.txt\n",
      "Resolving github.com (github.com)... 140.82.118.3\n",
      "Connecting to github.com (github.com)|140.82.118.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/html]\n",
      "Saving to: ‘chinese_stop_words.txt’\n",
      "\n",
      "chinese_stop_words.     [ <=>                ] 417.17K  2.17MB/s    in 0.2s    \n",
      "\n",
      "2020-02-28 13:10:51 (2.17 MB/s) - ‘chinese_stop_words.txt’ saved [427182]\n",
      "\n",
      "--2020-02-28 13:10:51--  http://vectors.nlpl.eu/repository/20/35.zip\n",
      "Resolving vectors.nlpl.eu (vectors.nlpl.eu)... 129.240.189.225\n",
      "Connecting to vectors.nlpl.eu (vectors.nlpl.eu)|129.240.189.225|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1458485917 (1.4G) [application/zip]\n",
      "Saving to: ‘zh.zip’\n",
      "\n",
      "zh.zip              100%[===================>]   1.36G  2.36MB/s    in 7m 44s  \n",
      "\n",
      "2020-02-28 13:18:35 (3.00 MB/s) - ‘zh.zip’ saved [1458485917/1458485917]\n",
      "\n",
      "Archive:  zh.zip\n",
      "  inflating: LIST                    \n",
      "  inflating: meta.json               \n",
      "  inflating: model.bin               \n",
      "  inflating: model.txt               \n",
      "  inflating: README                  \n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "import spacy\n",
    "\n",
    "#Embeddings\n",
    "word_dim = 100\n",
    "glove = torchtext.vocab.GloVe(name='6B', dim=word_dim)\n",
    "\n",
    "#tokenizer model\n",
    "nlp_en = spacy.load('en300')\n",
    "\n",
    "!wget -c https://github.com/Tony607/Chinese_sentiment_analysis/blob/master/data/chinese_stop_words.txt\n",
    "\n",
    "!wget -O zh.zip http://vectors.nlpl.eu/repository/20/35.zip\n",
    "\n",
    "!unzip zh.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "wv_from_bin = KeyedVectors.load_word2vec_format(\"model.bin\", binary=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-02-28 13:22:56--  https://competitions.codalab.org/my/datasets/download/03e23bd7-8084-4542-997b-6a1ca6dd8a5f\n",
      "Resolving competitions.codalab.org (competitions.codalab.org)... 129.175.22.230\n",
      "Connecting to competitions.codalab.org (competitions.codalab.org)|129.175.22.230|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 FOUND\n",
      "Location: https://newcodalab.lri.fr/prod-private/dataset_data_file/None/630ec/en-zh.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=cfeaedfc58e85372f4ecbe41b6f4a483f801b259eb521193a3418fe916d6c0ed&X-Amz-Date=20200228T132257Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20200228%2Fnewcodalab%2Fs3%2Faws4_request [following]\n",
      "--2020-02-28 13:22:57--  https://newcodalab.lri.fr/prod-private/dataset_data_file/None/630ec/en-zh.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=cfeaedfc58e85372f4ecbe41b6f4a483f801b259eb521193a3418fe916d6c0ed&X-Amz-Date=20200228T132257Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20200228%2Fnewcodalab%2Fs3%2Faws4_request\n",
      "Resolving newcodalab.lri.fr (newcodalab.lri.fr)... 129.175.15.11\n",
      "Connecting to newcodalab.lri.fr (newcodalab.lri.fr)|129.175.15.11|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 870893 (850K) [application/zip]\n",
      "Saving to: ‘enzh_data.zip’\n",
      "\n",
      "enzh_data.zip       100%[===================>] 850.48K  3.72MB/s    in 0.2s    \n",
      "\n",
      "2020-02-28 13:22:57 (3.72 MB/s) - ‘enzh_data.zip’ saved [870893/870893]\n",
      "\n",
      "Archive:  enzh_data.zip\n",
      "  inflating: dev.enzh.mt             \n",
      "  inflating: dev.enzh.scores         \n",
      "  inflating: dev.enzh.src            \n",
      "  inflating: test.enzh.mt            \n",
      "  inflating: test.enzh.src           \n",
      "  inflating: train.enzh.mt           \n",
      "  inflating: train.enzh.src          \n",
      "  inflating: train.enzh.scores       \n"
     ]
    }
   ],
   "source": [
    "from os.path import exists\n",
    "\n",
    "if not exists('enzh_data.zip'):\n",
    "    !wget -O enzh_data.zip https://competitions.codalab.org/my/datasets/download/03e23bd7-8084-4542-997b-6a1ca6dd8a5f\n",
    "    !unzip enzh_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import jieba\n",
    "import gensim \n",
    "import spacy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "stop_words = [line.rstrip() for line in open('chinese_stop_words.txt',\"r\", encoding=\"utf-8\") ]\n",
    "\n",
    "def get_sentence_vector_zh(line, mode):\n",
    "    vectors = []\n",
    "    for w in line:\n",
    "        try:\n",
    "            emb = wv_from_bin[w]\n",
    "            vectors.append(emb)\n",
    "        except:\n",
    "            pass #Do not add if the word is out of vocabulary\n",
    "    if vectors:\n",
    "        vectors = np.array(vectors)\n",
    "        # Outputs vector of shape (1)\n",
    "        if mode==0:\n",
    "            vectors = np.mean(vectors) \n",
    "        # Outputs vector of shape (word_dim)\n",
    "        elif mode==1:\n",
    "            vectors = np.mean(vectors, axis=0)\n",
    "        # Outputs vector of shape (len(line), word_dim)\n",
    "        elif mode==2:\n",
    "            vectors = vectors\n",
    "        return vectors\n",
    "    else:\n",
    "        if mode==0 or mode==1:\n",
    "            return np.zeros(word_dim)\n",
    "        elif mode==2:\n",
    "            return np.zeros((1, word_dim), dtype=np.float32)\n",
    "\n",
    "\n",
    "def processing_zh(sentence):\n",
    "    seg_list = jieba.lcut(sentence,cut_all=True)\n",
    "    doc = [word for word in seg_list if word not in stop_words]\n",
    "    docs = [e for e in doc if e.isalnum()]\n",
    "    return docs\n",
    "\n",
    "\n",
    "def get_sentence_embeddings_zh(f, mode=0):\n",
    "    file = open(f, encoding=\"utf8\") \n",
    "    lines = file.readlines() \n",
    "    sentences_vectors = []\n",
    "    for l in lines:\n",
    "        sent  = processing_zh(l)\n",
    "        vec = get_sentence_vector_zh(sent, mode=mode)\n",
    "        if vec is not None:\n",
    "            sentences_vectors.append(vec)\n",
    "        else:\n",
    "            print(l)\n",
    "    return sentences_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm \n",
    "\n",
    "stop_words_en = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(sentence,nlp):\n",
    "    text = sentence.lower()\n",
    "    doc = [token.lemma_ for token in  nlp.tokenizer(text)]\n",
    "    doc = [word for word in doc if word not in stop_words_en]\n",
    "    doc = [word for word in doc if word.isalpha()] #restricts string to alphabetic characters only\n",
    "    return doc\n",
    "\n",
    "def get_word_vector(embeddings, word):\n",
    "    try:\n",
    "        vec = embeddings.vectors[embeddings.stoi[word]]\n",
    "        return vec\n",
    "    except KeyError:\n",
    "        #print(f\"Word {word} does not exist\")\n",
    "        pass\n",
    "\n",
    "def get_sentence_vector(embeddings,line,mode):\n",
    "    vectors = []\n",
    "    for w in line:\n",
    "        emb = get_word_vector(embeddings,w)\n",
    "        #do not add if the word is out of vocabulary\n",
    "        if emb is not None:\n",
    "            vectors.append(emb)\n",
    "    # Outputs vector of shape (1)\n",
    "    if mode==0:\n",
    "        vectors = torch.mean(torch.stack(vectors))\n",
    "    # Outputs vector of shape (word_dim)\n",
    "    elif mode==1:\n",
    "        vectors = torch.mean(torch.stack(vectors), axis=0)\n",
    "    # Outputs vector of shape (len(line), word_dim)\n",
    "    elif mode==2:\n",
    "        vectors = torch.stack(vectors)\n",
    "    return vectors.data.numpy()\n",
    "\n",
    "\n",
    "def get_embeddings(f,embeddings,lang, mode=0):\n",
    "    file = open(f, encoding=\"utf8\") \n",
    "    lines = file.readlines() \n",
    "    sentences_vectors = []\n",
    "    for l in lines:\n",
    "        sentence = preprocess(l,lang)\n",
    "        try:\n",
    "            vec = get_sentence_vector(embeddings,sentence,mode)\n",
    "            sentences_vectors.append(vec)\n",
    "        except:\n",
    "            if mode == 0 or mode == 1:\n",
    "                sentences_vectors.append(np.zeros(word_dim))\n",
    "            if mode == 2:\n",
    "                print(\"Check if the dimension is correct\")\n",
    "                sentences_vectors.append(np.zeros((1, word_dim), dtype=np.float32))\n",
    "    return sentences_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.734 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "\n",
    "zh_train_mt = get_sentence_embeddings_zh(\"./train.enzh.mt\", mode=1)\n",
    "zh_train_src = get_embeddings(\"./train.enzh.src\", glove, nlp_en, mode=1)\n",
    "f_train_scores = open(\"./train.enzh.scores\",'r')\n",
    "zh_train_scores = f_train_scores.readlines()\n",
    "\n",
    "zh_val_src = get_embeddings(\"./dev.enzh.src\", glove, nlp_en, mode=1)\n",
    "zh_val_mt = get_sentence_embeddings_zh(\"./dev.enzh.mt\", mode=1)\n",
    "f_val_scores = open(\"./dev.enzh.scores\",'r')\n",
    "zh_val_scores = f_val_scores.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mt: 7000 Training src: 7000\n",
      "Validation mt: 1000 Validation src: 1000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training mt: {len(zh_train_mt)} Training src: {len(zh_train_src)}\")\n",
    "print(f\"Validation mt: {len(zh_val_mt)} Validation src: {len(zh_val_src)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_train = np.concatenate((np.asarray(zh_train_src), np.asarray(zh_train_mt)),axis=1) #[np.array(zh_train_src),np.array(zh_train_mt)]\n",
    "X_train_zh = np.array(X_train).transpose()\n",
    "\n",
    "X_val = np.concatenate((zh_val_src, zh_val_mt),axis=1) # [np.array(zh_val_src),np.array(zh_val_mt)]\n",
    "X_val_zh = np.array(X_val).transpose()\n",
    "\n",
    "#Scores\n",
    "train_scores = np.array(zh_train_scores).astype(float)\n",
    "y_train_zh = train_scores\n",
    "\n",
    "val_scores = np.array(zh_val_scores).astype(float)\n",
    "y_val_zh = val_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(predictions, targets):\n",
    "    res = np.sqrt(((predictions - targets) ** 2).mean())\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearsonr_loss(x, y):\n",
    "    mean_x = torch.mean(x)\n",
    "    mean_y = torch.mean(y)\n",
    "    xm = x.sub(mean_x)\n",
    "    ym = y.sub(mean_y)\n",
    "    r_num = xm.dot(ym)\n",
    "    r_den = torch.norm(xm, 2) * torch.norm(ym, 2)\n",
    "    r_val = r_num / r_den\n",
    "    return 1 - r_val**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(losses):\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(list(range(len(losses))), losses)\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training\")\n",
    "    plt.yscale('log')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear\n",
      "RMSE: 0.9043644819882596 Pearson: 0.3017330328685226 MAE: 0.6650792472344097\n",
      "\n",
      "poly\n",
      "RMSE: 0.9283952361365054 Pearson: 0.27498394599678677 MAE: 0.6657660489569661\n",
      "\n",
      "rbf\n",
      "RMSE: 0.898962013002422 Pearson: 0.34231180218064894 MAE: 0.6515368103586957\n",
      "\n",
      "sigmoid\n",
      "RMSE: 0.9104863313547815 Pearson: 0.3070833618658995 MAE: 0.6613085005674393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "for k in ['linear','poly','rbf','sigmoid']:\n",
    "    clf_t = SVR(kernel=k, gamma='scale')\n",
    "    clf_t.fit(X_train, y_train_zh)\n",
    "    print(k)\n",
    "    predictions = clf_t.predict(X_val)\n",
    "    pearson = pearsonr(y_val_zh, predictions)\n",
    "    print(f'RMSE: {rmse(predictions,y_val_zh)} Pearson: {pearson[0]} MAE: {mean_absolute_error(predictions,y_val_zh)}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 1000building tree 2 of 1000\n",
      "\n",
      "building tree 4 of 1000\n",
      "building tree 3 of 1000\n",
      "building tree 5 of 1000\n",
      "building tree 6 of 1000\n",
      "building tree 7 of 1000\n",
      "building tree 8 of 1000\n",
      "building tree 9 of 1000\n",
      "building tree 10 of 1000\n",
      "building tree 11 of 1000\n",
      "building tree 12 of 1000\n",
      "building tree 13 of 1000\n",
      "building tree 14 of 1000\n",
      "building tree 15 of 1000\n",
      "building tree 16 of 1000\n",
      "building tree 17 of 1000\n",
      "building tree 18 of 1000\n",
      "building tree 19 of 1000\n",
      "building tree 20 of 1000\n",
      "building tree 21 of 1000\n",
      "building tree 22 of 1000\n",
      "building tree 23 of 1000\n",
      "building tree 24 of 1000\n",
      "building tree 25 of 1000\n",
      "building tree 26 of 1000\n",
      "building tree 27 of 1000\n",
      "building tree 28 of 1000\n",
      "building tree 29 of 1000\n",
      "building tree 30 of 1000\n",
      "building tree 31 of 1000\n",
      "building tree 32 of 1000\n",
      "building tree 33 of 1000\n",
      "building tree 34 of 1000\n",
      "building tree 35 of 1000\n",
      "building tree 36 of 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   32.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 37 of 1000\n",
      "building tree 38 of 1000\n",
      "building tree 39 of 1000\n",
      "building tree 40 of 1000\n",
      "building tree 41 of 1000\n",
      "building tree 42 of 1000\n",
      "building tree 43 of 1000\n",
      "building tree 44 of 1000\n",
      "building tree 45 of 1000\n",
      "building tree 46 of 1000\n",
      "building tree 47 of 1000\n",
      "building tree 48 of 1000\n",
      "building tree 49 of 1000\n",
      "building tree 50 of 1000\n",
      "building tree 51 of 1000\n",
      "building tree 52 of 1000\n",
      "building tree 53 of 1000\n",
      "building tree 54 of 1000\n",
      "building tree 55 of 1000\n",
      "building tree 56 of 1000\n",
      "building tree 57 of 1000\n",
      "building tree 58 of 1000\n",
      "building tree 59 of 1000\n",
      "building tree 60 of 1000\n",
      "building tree 61 of 1000\n",
      "building tree 62 of 1000\n",
      "building tree 63 of 1000\n",
      "building tree 64 of 1000\n",
      "building tree 65 of 1000\n",
      "building tree 66 of 1000\n",
      "building tree 67 of 1000\n",
      "building tree 68 of 1000\n",
      "building tree 69 of 1000\n",
      "building tree 70 of 1000\n",
      "building tree 71 of 1000\n",
      "building tree 72 of 1000\n",
      "building tree 73 of 1000\n",
      "building tree 74 of 1000\n",
      "building tree 75 of 1000\n",
      "building tree 76 of 1000\n",
      "building tree 77 of 1000\n",
      "building tree 78 of 1000\n",
      "building tree 79 of 1000\n",
      "building tree 80 of 1000\n",
      "building tree 81 of 1000\n",
      "building tree 82 of 1000\n",
      "building tree 83 of 1000\n",
      "building tree 84 of 1000\n",
      "building tree 85 of 1000\n",
      "building tree 86 of 1000\n",
      "building tree 87 of 1000\n",
      "building tree 88 of 1000\n",
      "building tree 89 of 1000\n",
      "building tree 90 of 1000\n",
      "building tree 91 of 1000\n",
      "building tree 92 of 1000\n",
      "building tree 93 of 1000\n",
      "building tree 94 of 1000\n",
      "building tree 95 of 1000\n",
      "building tree 96 of 1000\n",
      "building tree 97 of 1000\n",
      "building tree 98 of 1000\n",
      "building tree 99 of 1000\n",
      "building tree 100 of 1000\n",
      "building tree 101 of 1000\n",
      "building tree 102 of 1000\n",
      "building tree 103 of 1000\n",
      "building tree 104 of 1000\n",
      "building tree 105 of 1000\n",
      "building tree 106 of 1000\n",
      "building tree 107 of 1000\n",
      "building tree 108 of 1000\n",
      "building tree 109 of 1000\n",
      "building tree 110 of 1000\n",
      "building tree 111 of 1000\n",
      "building tree 112 of 1000\n",
      "building tree 113 of 1000\n",
      "building tree 114 of 1000\n",
      "building tree 115 of 1000\n",
      "building tree 116 of 1000\n",
      "building tree 117 of 1000\n",
      "building tree 118 of 1000\n",
      "building tree 119 of 1000\n",
      "building tree 120 of 1000\n",
      "building tree 121 of 1000\n",
      "building tree 122 of 1000\n",
      "building tree 123 of 1000\n",
      "building tree 124 of 1000\n",
      "building tree 125 of 1000\n",
      "building tree 126 of 1000\n",
      "building tree 127 of 1000\n",
      "building tree 128 of 1000\n",
      "building tree 129 of 1000\n",
      "building tree 130 of 1000\n",
      "building tree 131 of 1000\n",
      "building tree 132 of 1000\n",
      "building tree 133 of 1000\n",
      "building tree 134 of 1000\n",
      "building tree 135 of 1000\n",
      "building tree 136 of 1000\n",
      "building tree 137 of 1000\n",
      "building tree 138 of 1000\n",
      "building tree 139 of 1000\n",
      "building tree 140 of 1000\n",
      "building tree 141 of 1000\n",
      "building tree 142 of 1000\n",
      "building tree 143 of 1000\n",
      "building tree 144 of 1000\n",
      "building tree 145 of 1000\n",
      "building tree 146 of 1000\n",
      "building tree 147 of 1000\n",
      "building tree 148 of 1000\n",
      "building tree 149 of 1000\n",
      "building tree 150 of 1000\n",
      "building tree 151 of 1000\n",
      "building tree 152 of 1000\n",
      "building tree 153 of 1000\n",
      "building tree 154 of 1000\n",
      "building tree 155 of 1000\n",
      "building tree 156 of 1000\n",
      "building tree 157 of 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  2.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 158 of 1000\n",
      "building tree 159 of 1000\n",
      "building tree 160 of 1000\n",
      "building tree 161 of 1000\n",
      "building tree 162 of 1000\n",
      "building tree 163 of 1000\n",
      "building tree 164 of 1000\n",
      "building tree 165 of 1000\n",
      "building tree 166 of 1000\n",
      "building tree 167 of 1000\n",
      "building tree 168 of 1000\n",
      "building tree 169 of 1000\n",
      "building tree 170 of 1000\n",
      "building tree 171 of 1000\n",
      "building tree 172 of 1000\n",
      "building tree 173 of 1000\n",
      "building tree 174 of 1000\n",
      "building tree 175 of 1000\n",
      "building tree 176 of 1000\n",
      "building tree 177 of 1000\n",
      "building tree 178 of 1000\n",
      "building tree 179 of 1000\n",
      "building tree 180 of 1000\n",
      "building tree 181 of 1000\n",
      "building tree 182 of 1000\n",
      "building tree 183 of 1000\n",
      "building tree 184 of 1000\n",
      "building tree 185 of 1000\n",
      "building tree 186 of 1000\n",
      "building tree 187 of 1000\n",
      "building tree 188 of 1000\n",
      "building tree 189 of 1000\n",
      "building tree 190 of 1000\n",
      "building tree 191 of 1000\n",
      "building tree 192 of 1000\n",
      "building tree 193 of 1000\n",
      "building tree 194 of 1000\n",
      "building tree 195 of 1000\n",
      "building tree 196 of 1000\n",
      "building tree 197 of 1000\n",
      "building tree 198 of 1000\n",
      "building tree 199 of 1000\n",
      "building tree 200 of 1000\n",
      "building tree 201 of 1000\n",
      "building tree 202 of 1000\n",
      "building tree 203 of 1000\n",
      "building tree 204 of 1000\n",
      "building tree 205 of 1000\n",
      "building tree 206 of 1000\n",
      "building tree 207 of 1000\n",
      "building tree 208 of 1000\n",
      "building tree 209 of 1000\n",
      "building tree 210 of 1000\n",
      "building tree 211 of 1000\n",
      "building tree 212 of 1000\n",
      "building tree 213 of 1000\n",
      "building tree 214 of 1000\n",
      "building tree 215 of 1000\n",
      "building tree 216 of 1000\n",
      "building tree 217 of 1000\n",
      "building tree 218 of 1000\n",
      "building tree 219 of 1000\n",
      "building tree 220 of 1000\n",
      "building tree 221 of 1000\n",
      "building tree 222 of 1000\n",
      "building tree 223 of 1000\n",
      "building tree 224 of 1000\n",
      "building tree 225 of 1000\n",
      "building tree 226 of 1000\n",
      "building tree 227 of 1000\n",
      "building tree 228 of 1000\n",
      "building tree 229 of 1000\n",
      "building tree 230 of 1000\n",
      "building tree 231 of 1000\n",
      "building tree 232 of 1000\n",
      "building tree 233 of 1000\n",
      "building tree 234 of 1000\n",
      "building tree 235 of 1000\n",
      "building tree 236 of 1000\n",
      "building tree 237 of 1000\n",
      "building tree 238 of 1000\n",
      "building tree 239 of 1000\n",
      "building tree 240 of 1000\n",
      "building tree 241 of 1000\n",
      "building tree 242 of 1000\n",
      "building tree 243 of 1000\n",
      "building tree 244 of 1000\n",
      "building tree 245 of 1000\n",
      "building tree 246 of 1000\n",
      "building tree 247 of 1000\n",
      "building tree 248 of 1000\n",
      "building tree 249 of 1000\n",
      "building tree 250 of 1000\n",
      "building tree 251 of 1000\n",
      "building tree 252 of 1000\n",
      "building tree 253 of 1000\n",
      "building tree 254 of 1000\n",
      "building tree 255 of 1000\n",
      "building tree 256 of 1000\n",
      "building tree 257 of 1000\n",
      "building tree 258 of 1000\n",
      "building tree 259 of 1000\n",
      "building tree 260 of 1000\n",
      "building tree 261 of 1000\n",
      "building tree 262 of 1000\n",
      "building tree 263 of 1000\n",
      "building tree 264 of 1000\n",
      "building tree 265 of 1000\n",
      "building tree 266 of 1000\n",
      "building tree 267 of 1000\n",
      "building tree 268 of 1000\n",
      "building tree 269 of 1000\n",
      "building tree 270 of 1000\n",
      "building tree 271 of 1000\n",
      "building tree 272 of 1000\n",
      "building tree 273 of 1000\n",
      "building tree 274 of 1000\n",
      "building tree 275 of 1000\n",
      "building tree 276 of 1000\n",
      "building tree 277 of 1000\n",
      "building tree 278 of 1000\n",
      "building tree 279 of 1000\n",
      "building tree 280 of 1000\n",
      "building tree 281 of 1000\n",
      "building tree 282 of 1000\n",
      "building tree 283 of 1000\n",
      "building tree 284 of 1000\n",
      "building tree 285 of 1000\n",
      "building tree 286 of 1000\n",
      "building tree 287 of 1000\n",
      "building tree 288 of 1000\n",
      "building tree 289 of 1000\n",
      "building tree 290 of 1000\n",
      "building tree 291 of 1000\n",
      "building tree 292 of 1000\n",
      "building tree 293 of 1000\n",
      "building tree 294 of 1000\n",
      "building tree 295 of 1000\n",
      "building tree 296 of 1000\n",
      "building tree 297 of 1000\n",
      "building tree 298 of 1000\n",
      "building tree 299 of 1000\n",
      "building tree 300 of 1000\n",
      "building tree 301 of 1000\n",
      "building tree 302 of 1000\n",
      "building tree 303 of 1000\n",
      "building tree 304 of 1000\n",
      "building tree 305 of 1000\n",
      "building tree 306 of 1000\n",
      "building tree 307 of 1000\n",
      "building tree 308 of 1000\n",
      "building tree 309 of 1000\n",
      "building tree 310 of 1000\n",
      "building tree 311 of 1000\n",
      "building tree 312 of 1000\n",
      "building tree 313 of 1000\n",
      "building tree 314 of 1000\n",
      "building tree 315 of 1000\n",
      "building tree 316 of 1000\n",
      "building tree 317 of 1000\n",
      "building tree 318 of 1000\n",
      "building tree 319 of 1000\n",
      "building tree 320 of 1000\n",
      "building tree 321 of 1000\n",
      "building tree 322 of 1000\n",
      "building tree 323 of 1000\n",
      "building tree 324 of 1000\n",
      "building tree 325 of 1000\n",
      "building tree 326 of 1000\n",
      "building tree 327 of 1000\n",
      "building tree 328 of 1000\n",
      "building tree 329 of 1000\n",
      "building tree 330 of 1000\n",
      "building tree 331 of 1000\n",
      "building tree 332 of 1000\n",
      "building tree 333 of 1000\n",
      "building tree 334 of 1000\n",
      "building tree 335 of 1000\n",
      "building tree 336 of 1000\n",
      "building tree 337 of 1000\n",
      "building tree 338 of 1000\n",
      "building tree 339 of 1000\n",
      "building tree 340 of 1000\n",
      "building tree 341 of 1000\n",
      "building tree 342 of 1000\n",
      "building tree 343 of 1000\n",
      "building tree 344 of 1000\n",
      "building tree 345 of 1000\n",
      "building tree 346 of 1000\n",
      "building tree 347 of 1000\n",
      "building tree 348 of 1000\n",
      "building tree 349 of 1000\n",
      "building tree 350 of 1000\n",
      "building tree 351 of 1000\n",
      "building tree 352 of 1000\n",
      "building tree 353 of 1000\n",
      "building tree 354 of 1000\n",
      "building tree 355 of 1000\n",
      "building tree 356 of 1000\n",
      "building tree 357 of 1000\n",
      "building tree 358 of 1000\n",
      "building tree 359 of 1000\n",
      "building tree 360 of 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:  5.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 361 of 1000\n",
      "building tree 362 of 1000\n",
      "building tree 363 of 1000\n",
      "building tree 364 of 1000\n",
      "building tree 365 of 1000\n",
      "building tree 366 of 1000\n",
      "building tree 367 of 1000\n",
      "building tree 368 of 1000\n",
      "building tree 369 of 1000\n",
      "building tree 370 of 1000\n",
      "building tree 371 of 1000\n",
      "building tree 372 of 1000\n",
      "building tree 373 of 1000\n",
      "building tree 374 of 1000\n",
      "building tree 375 of 1000\n",
      "building tree 376 of 1000\n",
      "building tree 377 of 1000\n",
      "building tree 378 of 1000\n",
      "building tree 379 of 1000\n",
      "building tree 380 of 1000\n",
      "building tree 381 of 1000\n",
      "building tree 382 of 1000\n",
      "building tree 383 of 1000\n",
      "building tree 384 of 1000\n",
      "building tree 385 of 1000\n",
      "building tree 386 of 1000\n",
      "building tree 387 of 1000\n",
      "building tree 388 of 1000\n",
      "building tree 389 of 1000\n",
      "building tree 390 of 1000\n",
      "building tree 391 of 1000\n",
      "building tree 392 of 1000\n",
      "building tree 393 of 1000\n",
      "building tree 394 of 1000\n",
      "building tree 395 of 1000\n",
      "building tree 396 of 1000\n",
      "building tree 397 of 1000\n",
      "building tree 398 of 1000\n",
      "building tree 399 of 1000\n",
      "building tree 400 of 1000\n",
      "building tree 401 of 1000\n",
      "building tree 402 of 1000\n",
      "building tree 403 of 1000\n",
      "building tree 404 of 1000\n",
      "building tree 405 of 1000\n",
      "building tree 406 of 1000\n",
      "building tree 407 of 1000\n",
      "building tree 408 of 1000\n",
      "building tree 409 of 1000\n",
      "building tree 410 of 1000\n",
      "building tree 411 of 1000\n",
      "building tree 412 of 1000\n",
      "building tree 413 of 1000\n",
      "building tree 414 of 1000\n",
      "building tree 415 of 1000\n",
      "building tree 416 of 1000\n",
      "building tree 417 of 1000\n",
      "building tree 418 of 1000\n",
      "building tree 419 of 1000\n",
      "building tree 420 of 1000\n",
      "building tree 421 of 1000\n",
      "building tree 422 of 1000\n",
      "building tree 423 of 1000\n",
      "building tree 424 of 1000\n",
      "building tree 425 of 1000\n",
      "building tree 426 of 1000\n",
      "building tree 427 of 1000\n",
      "building tree 428 of 1000\n",
      "building tree 429 of 1000\n",
      "building tree 430 of 1000\n",
      "building tree 431 of 1000\n",
      "building tree 432 of 1000\n",
      "building tree 433 of 1000\n",
      "building tree 434 of 1000\n",
      "building tree 435 of 1000\n",
      "building tree 436 of 1000\n",
      "building tree 437 of 1000\n",
      "building tree 438 of 1000\n",
      "building tree 439 of 1000\n",
      "building tree 440 of 1000\n",
      "building tree 441 of 1000building tree 442 of 1000\n",
      "\n",
      "building tree 443 of 1000\n",
      "building tree 444 of 1000\n",
      "building tree 445 of 1000\n",
      "building tree 446 of 1000\n",
      "building tree 447 of 1000\n",
      "building tree 448 of 1000\n",
      "building tree 449 of 1000\n",
      "building tree 450 of 1000\n",
      "building tree 451 of 1000\n",
      "building tree 452 of 1000\n",
      "building tree 453 of 1000\n",
      "building tree 454 of 1000\n",
      "building tree 455 of 1000\n",
      "building tree 456 of 1000\n",
      "building tree 457 of 1000\n",
      "building tree 458 of 1000\n",
      "building tree 459 of 1000\n",
      "building tree 460 of 1000\n",
      "building tree 461 of 1000\n",
      "building tree 462 of 1000\n",
      "building tree 463 of 1000\n",
      "building tree 464 of 1000\n",
      "building tree 465 of 1000\n",
      "building tree 466 of 1000\n",
      "building tree 467 of 1000\n",
      "building tree 468 of 1000\n",
      "building tree 469 of 1000\n",
      "building tree 470 of 1000\n",
      "building tree 471 of 1000\n",
      "building tree 472 of 1000\n",
      "building tree 473 of 1000\n",
      "building tree 474 of 1000\n",
      "building tree 475 of 1000\n",
      "building tree 476 of 1000\n",
      "building tree 477 of 1000\n",
      "building tree 478 of 1000\n",
      "building tree 479 of 1000\n",
      "building tree 480 of 1000\n",
      "building tree 481 of 1000\n",
      "building tree 482 of 1000\n",
      "building tree 483 of 1000\n",
      "building tree 484 of 1000\n",
      "building tree 485 of 1000\n",
      "building tree 486 of 1000\n",
      "building tree 487 of 1000\n",
      "building tree 488 of 1000\n",
      "building tree 489 of 1000\n",
      "building tree 490 of 1000\n",
      "building tree 491 of 1000\n",
      "building tree 492 of 1000\n",
      "building tree 493 of 1000\n",
      "building tree 494 of 1000\n",
      "building tree 495 of 1000\n",
      "building tree 496 of 1000\n",
      "building tree 497 of 1000\n",
      "building tree 498 of 1000\n",
      "building tree 499 of 1000\n",
      "building tree 500 of 1000\n",
      "building tree 501 of 1000\n",
      "building tree 502 of 1000\n",
      "building tree 503 of 1000\n",
      "building tree 504 of 1000\n",
      "building tree 505 of 1000\n",
      "building tree 506 of 1000\n",
      "building tree 507 of 1000\n",
      "building tree 508 of 1000\n",
      "building tree 509 of 1000\n",
      "building tree 510 of 1000\n",
      "building tree 511 of 1000\n",
      "building tree 512 of 1000\n",
      "building tree 513 of 1000\n",
      "building tree 514 of 1000\n",
      "building tree 515 of 1000\n",
      "building tree 516 of 1000\n",
      "building tree 517 of 1000\n",
      "building tree 518 of 1000\n",
      "building tree 519 of 1000\n",
      "building tree 520 of 1000\n",
      "building tree 521 of 1000\n",
      "building tree 522 of 1000\n",
      "building tree 523 of 1000\n",
      "building tree 524 of 1000\n",
      "building tree 525 of 1000\n",
      "building tree 526 of 1000\n",
      "building tree 527 of 1000\n",
      "building tree 528 of 1000\n",
      "building tree 529 of 1000\n",
      "building tree 530 of 1000\n",
      "building tree 531 of 1000\n",
      "building tree 532 of 1000\n",
      "building tree 533 of 1000\n",
      "building tree 534 of 1000\n",
      "building tree 535 of 1000\n",
      "building tree 536 of 1000\n",
      "building tree 537 of 1000\n",
      "building tree 538 of 1000\n",
      "building tree 539 of 1000\n",
      "building tree 540 of 1000\n",
      "building tree 541 of 1000\n",
      "building tree 542 of 1000\n",
      "building tree 543 of 1000\n",
      "building tree 544 of 1000\n",
      "building tree 545 of 1000\n",
      "building tree 546 of 1000\n",
      "building tree 547 of 1000\n",
      "building tree 548 of 1000\n",
      "building tree 549 of 1000\n",
      "building tree 550 of 1000\n",
      "building tree 551 of 1000\n",
      "building tree 552 of 1000\n",
      "building tree 553 of 1000\n",
      "building tree 554 of 1000\n",
      "building tree 555 of 1000\n",
      "building tree 556 of 1000\n",
      "building tree 557 of 1000\n",
      "building tree 558 of 1000\n",
      "building tree 559 of 1000\n",
      "building tree 560 of 1000\n",
      "building tree 561 of 1000\n",
      "building tree 562 of 1000\n",
      "building tree 563 of 1000\n",
      "building tree 564 of 1000\n",
      "building tree 565 of 1000\n",
      "building tree 566 of 1000\n",
      "building tree 567 of 1000\n",
      "building tree 568 of 1000\n",
      "building tree 569 of 1000\n",
      "building tree 570 of 1000\n",
      "building tree 571 of 1000\n",
      "building tree 572 of 1000\n",
      "building tree 573 of 1000\n",
      "building tree 574 of 1000\n",
      "building tree 575 of 1000\n",
      "building tree 576 of 1000\n",
      "building tree 577 of 1000\n",
      "building tree 578 of 1000\n",
      "building tree 579 of 1000\n",
      "building tree 580 of 1000\n",
      "building tree 581 of 1000\n",
      "building tree 582 of 1000\n",
      "building tree 583 of 1000\n",
      "building tree 584 of 1000\n",
      "building tree 585 of 1000\n",
      "building tree 586 of 1000\n",
      "building tree 587 of 1000\n",
      "building tree 588 of 1000\n",
      "building tree 589 of 1000\n",
      "building tree 590 of 1000\n",
      "building tree 591 of 1000\n",
      "building tree 592 of 1000\n",
      "building tree 593 of 1000\n",
      "building tree 594 of 1000\n",
      "building tree 595 of 1000\n",
      "building tree 596 of 1000\n",
      "building tree 597 of 1000\n",
      "building tree 598 of 1000\n",
      "building tree 599 of 1000\n",
      "building tree 600 of 1000\n",
      "building tree 601 of 1000\n",
      "building tree 602 of 1000\n",
      "building tree 603 of 1000\n",
      "building tree 604 of 1000\n",
      "building tree 605 of 1000\n",
      "building tree 606 of 1000\n",
      "building tree 607 of 1000\n",
      "building tree 608 of 1000\n",
      "building tree 609 of 1000\n",
      "building tree 610 of 1000\n",
      "building tree 611 of 1000\n",
      "building tree 612 of 1000\n",
      "building tree 613 of 1000\n",
      "building tree 614 of 1000\n",
      "building tree 615 of 1000\n",
      "building tree 616 of 1000\n",
      "building tree 617 of 1000\n",
      "building tree 618 of 1000\n",
      "building tree 619 of 1000\n",
      "building tree 620 of 1000\n",
      "building tree 621 of 1000\n",
      "building tree 622 of 1000\n",
      "building tree 623 of 1000\n",
      "building tree 624 of 1000\n",
      "building tree 625 of 1000\n",
      "building tree 626 of 1000\n",
      "building tree 627 of 1000\n",
      "building tree 628 of 1000\n",
      "building tree 629 of 1000\n",
      "building tree 630 of 1000\n",
      "building tree 631 of 1000\n",
      "building tree 632 of 1000\n",
      "building tree 633 of 1000\n",
      "building tree 634 of 1000\n",
      "building tree 635 of 1000\n",
      "building tree 636 of 1000\n",
      "building tree 637 of 1000\n",
      "building tree 638 of 1000\n",
      "building tree 639 of 1000\n",
      "building tree 640 of 1000\n",
      "building tree 641 of 1000\n",
      "building tree 642 of 1000\n",
      "building tree 643 of 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed:  8.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 644 of 1000\n",
      "building tree 645 of 1000\n",
      "building tree 646 of 1000\n",
      "building tree 647 of 1000\n",
      "building tree 648 of 1000\n",
      "building tree 649 of 1000\n",
      "building tree 650 of 1000\n",
      "building tree 651 of 1000\n",
      "building tree 652 of 1000\n",
      "building tree 653 of 1000\n",
      "building tree 654 of 1000\n",
      "building tree 655 of 1000\n",
      "building tree 656 of 1000\n",
      "building tree 657 of 1000\n",
      "building tree 658 of 1000\n",
      "building tree 659 of 1000\n",
      "building tree 660 of 1000\n",
      "building tree 661 of 1000\n",
      "building tree 662 of 1000\n",
      "building tree 663 of 1000\n",
      "building tree 664 of 1000\n",
      "building tree 665 of 1000\n",
      "building tree 666 of 1000\n",
      "building tree 667 of 1000\n",
      "building tree 668 of 1000\n",
      "building tree 669 of 1000\n",
      "building tree 670 of 1000\n",
      "building tree 671 of 1000\n",
      "building tree 672 of 1000\n",
      "building tree 673 of 1000\n",
      "building tree 674 of 1000\n",
      "building tree 675 of 1000\n",
      "building tree 676 of 1000\n",
      "building tree 677 of 1000\n",
      "building tree 678 of 1000\n",
      "building tree 679 of 1000\n",
      "building tree 680 of 1000\n",
      "building tree 681 of 1000\n",
      "building tree 682 of 1000\n",
      "building tree 683 of 1000\n",
      "building tree 684 of 1000\n",
      "building tree 685 of 1000\n",
      "building tree 686 of 1000\n",
      "building tree 687 of 1000\n",
      "building tree 688 of 1000\n",
      "building tree 689 of 1000\n",
      "building tree 690 of 1000\n",
      "building tree 691 of 1000\n",
      "building tree 692 of 1000\n",
      "building tree 693 of 1000\n",
      "building tree 694 of 1000\n",
      "building tree 695 of 1000\n",
      "building tree 696 of 1000\n",
      "building tree 697 of 1000\n",
      "building tree 698 of 1000\n",
      "building tree 699 of 1000\n",
      "building tree 700 of 1000\n",
      "building tree 701 of 1000\n",
      "building tree 702 of 1000\n",
      "building tree 703 of 1000\n",
      "building tree 704 of 1000\n",
      "building tree 705 of 1000\n",
      "building tree 706 of 1000\n",
      "building tree 707 of 1000\n",
      "building tree 708 of 1000\n",
      "building tree 709 of 1000\n",
      "building tree 710 of 1000\n",
      "building tree 711 of 1000\n",
      "building tree 712 of 1000\n",
      "building tree 713 of 1000\n",
      "building tree 714 of 1000\n",
      "building tree 715 of 1000\n",
      "building tree 716 of 1000\n",
      "building tree 717 of 1000\n",
      "building tree 718 of 1000\n",
      "building tree 719 of 1000\n",
      "building tree 720 of 1000\n",
      "building tree 721 of 1000\n",
      "building tree 722 of 1000\n",
      "building tree 723 of 1000\n",
      "building tree 724 of 1000\n",
      "building tree 725 of 1000\n",
      "building tree 726 of 1000\n",
      "building tree 727 of 1000\n",
      "building tree 728 of 1000\n",
      "building tree 729 of 1000\n",
      "building tree 730 of 1000\n",
      "building tree 731 of 1000\n",
      "building tree 732 of 1000\n",
      "building tree 733 of 1000\n",
      "building tree 734 of 1000\n",
      "building tree 735 of 1000\n",
      "building tree 736 of 1000\n",
      "building tree 737 of 1000\n",
      "building tree 738 of 1000\n",
      "building tree 739 of 1000\n",
      "building tree 740 of 1000\n",
      "building tree 741 of 1000\n",
      "building tree 742 of 1000\n",
      "building tree 743 of 1000\n",
      "building tree 744 of 1000\n",
      "building tree 745 of 1000\n",
      "building tree 746 of 1000\n",
      "building tree 747 of 1000\n",
      "building tree 748 of 1000\n",
      "building tree 749 of 1000\n",
      "building tree 750 of 1000\n",
      "building tree 751 of 1000\n",
      "building tree 752 of 1000\n",
      "building tree 753 of 1000\n",
      "building tree 754 of 1000\n",
      "building tree 755 of 1000\n",
      "building tree 756 of 1000\n",
      "building tree 757 of 1000\n",
      "building tree 758 of 1000\n",
      "building tree 759 of 1000\n",
      "building tree 760 of 1000\n",
      "building tree 761 of 1000\n",
      "building tree 762 of 1000\n",
      "building tree 763 of 1000\n",
      "building tree 764 of 1000\n",
      "building tree 765 of 1000\n",
      "building tree 766 of 1000\n",
      "building tree 767 of 1000\n",
      "building tree 768 of 1000\n",
      "building tree 769 of 1000\n",
      "building tree 770 of 1000\n",
      "building tree 771 of 1000\n",
      "building tree 772 of 1000\n",
      "building tree 773 of 1000\n",
      "building tree 774 of 1000\n",
      "building tree 775 of 1000\n",
      "building tree 776 of 1000\n",
      "building tree 777 of 1000\n",
      "building tree 778 of 1000\n",
      "building tree 779 of 1000\n",
      "building tree 780 of 1000\n",
      "building tree 781 of 1000\n",
      "building tree 782 of 1000\n",
      "building tree 783 of 1000\n",
      "building tree 784 of 1000\n",
      "building tree 785 of 1000\n",
      "building tree 786 of 1000\n",
      "building tree 787 of 1000\n",
      "building tree 788 of 1000\n",
      "building tree 789 of 1000\n",
      "building tree 790 of 1000\n",
      "building tree 791 of 1000\n",
      "building tree 792 of 1000\n",
      "building tree 793 of 1000\n",
      "building tree 794 of 1000\n",
      "building tree 795 of 1000\n",
      "building tree 796 of 1000\n",
      "building tree 797 of 1000\n",
      "building tree 798 of 1000\n",
      "building tree 799 of 1000\n",
      "building tree 800 of 1000\n",
      "building tree 801 of 1000\n",
      "building tree 802 of 1000\n",
      "building tree 803 of 1000\n",
      "building tree 804 of 1000\n",
      "building tree 805 of 1000\n",
      "building tree 806 of 1000\n",
      "building tree 807 of 1000\n",
      "building tree 808 of 1000\n",
      "building tree 809 of 1000\n",
      "building tree 810 of 1000\n",
      "building tree 811 of 1000\n",
      "building tree 812 of 1000\n",
      "building tree 813 of 1000\n",
      "building tree 814 of 1000\n",
      "building tree 815 of 1000\n",
      "building tree 816 of 1000\n",
      "building tree 817 of 1000\n",
      "building tree 818 of 1000\n",
      "building tree 819 of 1000\n",
      "building tree 820 of 1000\n",
      "building tree 821 of 1000\n",
      "building tree 822 of 1000\n",
      "building tree 823 of 1000\n",
      "building tree 824 of 1000\n",
      "building tree 825 of 1000\n",
      "building tree 826 of 1000\n",
      "building tree 827 of 1000\n",
      "building tree 828 of 1000\n",
      "building tree 829 of 1000\n",
      "building tree 830 of 1000\n",
      "building tree 831 of 1000\n",
      "building tree 832 of 1000\n",
      "building tree 833 of 1000\n",
      "building tree 834 of 1000\n",
      "building tree 835 of 1000\n",
      "building tree 836 of 1000\n",
      "building tree 837 of 1000\n",
      "building tree 838 of 1000\n",
      "building tree 839 of 1000\n",
      "building tree 840 of 1000\n",
      "building tree 841 of 1000\n",
      "building tree 842 of 1000\n",
      "building tree 843 of 1000\n",
      "building tree 844 of 1000\n",
      "building tree 845 of 1000\n",
      "building tree 846 of 1000\n",
      "building tree 847 of 1000\n",
      "building tree 848 of 1000\n",
      "building tree 849 of 1000\n",
      "building tree 850 of 1000\n",
      "building tree 851 of 1000\n",
      "building tree 852 of 1000\n",
      "building tree 853 of 1000\n",
      "building tree 854 of 1000\n",
      "building tree 855 of 1000\n",
      "building tree 856 of 1000\n",
      "building tree 857 of 1000\n",
      "building tree 858 of 1000\n",
      "building tree 859 of 1000\n",
      "building tree 860 of 1000\n",
      "building tree 861 of 1000\n",
      "building tree 862 of 1000\n",
      "building tree 863 of 1000\n",
      "building tree 864 of 1000\n",
      "building tree 865 of 1000\n",
      "building tree 866 of 1000\n",
      "building tree 867 of 1000\n",
      "building tree 868 of 1000\n",
      "building tree 869 of 1000\n",
      "building tree 870 of 1000\n",
      "building tree 871 of 1000\n",
      "building tree 872 of 1000\n",
      "building tree 873 of 1000\n",
      "building tree 874 of 1000\n",
      "building tree 875 of 1000\n",
      "building tree 876 of 1000\n",
      "building tree 877 of 1000\n",
      "building tree 878 of 1000\n",
      "building tree 879 of 1000\n",
      "building tree 880 of 1000\n",
      "building tree 881 of 1000\n",
      "building tree 882 of 1000\n",
      "building tree 883 of 1000\n",
      "building tree 884 of 1000\n",
      "building tree 885 of 1000\n",
      "building tree 886 of 1000\n",
      "building tree 887 of 1000\n",
      "building tree 888 of 1000\n",
      "building tree 889 of 1000\n",
      "building tree 890 of 1000\n",
      "building tree 891 of 1000\n",
      "building tree 892 of 1000\n",
      "building tree 893 of 1000\n",
      "building tree 894 of 1000\n",
      "building tree 895 of 1000\n",
      "building tree 896 of 1000\n",
      "building tree 897 of 1000\n",
      "building tree 898 of 1000\n",
      "building tree 899 of 1000\n",
      "building tree 900 of 1000\n",
      "building tree 901 of 1000\n",
      "building tree 902 of 1000\n",
      "building tree 903 of 1000\n",
      "building tree 904 of 1000\n",
      "building tree 905 of 1000\n",
      "building tree 906 of 1000\n",
      "building tree 907 of 1000\n",
      "building tree 908 of 1000\n",
      "building tree 909 of 1000\n",
      "building tree 910 of 1000\n",
      "building tree 911 of 1000\n",
      "building tree 912 of 1000\n",
      "building tree 913 of 1000\n",
      "building tree 914 of 1000\n",
      "building tree 915 of 1000\n",
      "building tree 916 of 1000\n",
      "building tree 917 of 1000\n",
      "building tree 918 of 1000\n",
      "building tree 919 of 1000\n",
      "building tree 920 of 1000\n",
      "building tree 921 of 1000\n",
      "building tree 922 of 1000\n",
      "building tree 923 of 1000\n",
      "building tree 924 of 1000\n",
      "building tree 925 of 1000\n",
      "building tree 926 of 1000\n",
      "building tree 927 of 1000\n",
      "building tree 928 of 1000\n",
      "building tree 929 of 1000\n",
      "building tree 930 of 1000\n",
      "building tree 931 of 1000\n",
      "building tree 932 of 1000\n",
      "building tree 933 of 1000\n",
      "building tree 934 of 1000\n",
      "building tree 935 of 1000\n",
      "building tree 936 of 1000\n",
      "building tree 937 of 1000\n",
      "building tree 938 of 1000\n",
      "building tree 939 of 1000\n",
      "building tree 940 of 1000\n",
      "building tree 941 of 1000\n",
      "building tree 942 of 1000\n",
      "building tree 943 of 1000\n",
      "building tree 944 of 1000\n",
      "building tree 945 of 1000\n",
      "building tree 946 of 1000\n",
      "building tree 947 of 1000\n",
      "building tree 948 of 1000\n",
      "building tree 949 of 1000\n",
      "building tree 950 of 1000\n",
      "building tree 951 of 1000\n",
      "building tree 952 of 1000\n",
      "building tree 953 of 1000\n",
      "building tree 954 of 1000\n",
      "building tree 955 of 1000\n",
      "building tree 956 of 1000\n",
      "building tree 957 of 1000\n",
      "building tree 958 of 1000\n",
      "building tree 959 of 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 960 of 1000\n",
      "building tree 961 of 1000\n",
      "building tree 962 of 1000\n",
      "building tree 963 of 1000\n",
      "building tree 964 of 1000\n",
      "building tree 965 of 1000\n",
      "building tree 966 of 1000\n",
      "building tree 967 of 1000\n",
      "building tree 968 of 1000\n",
      "building tree 969 of 1000\n",
      "building tree 970 of 1000\n",
      "building tree 971 of 1000\n",
      "building tree 972 of 1000\n",
      "building tree 973 of 1000\n",
      "building tree 974 of 1000\n",
      "building tree 975 of 1000\n",
      "building tree 976 of 1000\n",
      "building tree 977 of 1000\n",
      "building tree 978 of 1000\n",
      "building tree 979 of 1000\n",
      "building tree 980 of 1000\n",
      "building tree 981 of 1000\n",
      "building tree 982 of 1000\n",
      "building tree 983 of 1000\n",
      "building tree 984 of 1000\n",
      "building tree 985 of 1000\n",
      "building tree 986 of 1000\n",
      "building tree 987 of 1000\n",
      "building tree 988 of 1000\n",
      "building tree 989 of 1000\n",
      "building tree 990 of 1000\n",
      "building tree 991 of 1000\n",
      "building tree 992 of 1000\n",
      "building tree 993 of 1000\n",
      "building tree 994 of 1000\n",
      "building tree 995 of 1000\n",
      "building tree 996 of 1000\n",
      "building tree 997 of 1000\n",
      "building tree 998 of 1000\n",
      "building tree 999 of 1000\n",
      "building tree 1000 of 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed: 13.1min finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 154 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 357 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 640 tasks      | elapsed:    0.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.8781294394897398\n",
      "Pearson 0.26165584789078766\n",
      "MAE: 0.7111023870525853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 1000 out of 1000 | elapsed:    0.5s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 0, verbose=2, n_jobs=-1)\n",
    "rf.fit(X_train, y_train_zh);\n",
    "predictions = rf.predict(X_val)\n",
    "\n",
    "pearson = pearsonr(y_val_zh, predictions)\n",
    "print('RMSE:', rmse(predictions,y_val_zh))\n",
    "print(f\"Pearson {pearson[0]}\")\n",
    "print(f\"MAE: {mean_absolute_error(predictions,y_val_zh)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(n_feature, 256)  \n",
    "        self.hidden2 = torch.nn.Linear(256, 64)\n",
    "        self.hidden3 = torch.nn.Linear(64, 16)\n",
    "        self.predict = torch.nn.Linear(16, n_output) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden(x))  \n",
    "        x = F.relu(self.hidden2(x))  \n",
    "        x = F.relu(self.hidden3(x))  \n",
    "        x = self.predict(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 RMSE: 0.9077072059207034 Pearson: 0.17766692783839874 MAE: 0.7313784657400801\n",
      "Epoch: 2 RMSE: 0.9068139992495907 Pearson: 0.20887013953281763 MAE: 0.7301811280956735\n",
      "Epoch: 3 RMSE: 0.9062328645191146 Pearson: 0.21781361176081487 MAE: 0.7237038092151103\n",
      "Epoch: 4 RMSE: 0.9048332498279009 Pearson: 0.22117140885449388 MAE: 0.7252941498402408\n",
      "Epoch: 5 RMSE: 0.9032574236893159 Pearson: 0.22234807796597586 MAE: 0.7249556483587289\n",
      "Epoch: 6 RMSE: 0.9017668457138301 Pearson: 0.22233266134679808 MAE: 0.7181607598797866\n",
      "Epoch: 7 RMSE: 0.8992172087049646 Pearson: 0.2221676036085374 MAE: 0.7171219725812009\n",
      "Epoch: 8 RMSE: 0.8974306250042172 Pearson: 0.22179107380484553 MAE: 0.7075998626460352\n",
      "Epoch: 9 RMSE: 0.8935012913144164 Pearson: 0.2240215956555864 MAE: 0.7088743018157144\n",
      "Epoch: 10 RMSE: 0.8905460716612555 Pearson: 0.22690847511706377 MAE: 0.7052935248852684\n",
      "Epoch: 11 RMSE: 0.8887392148618108 Pearson: 0.23179649182964007 MAE: 0.6977704793522658\n",
      "Epoch: 12 RMSE: 0.8856000323161011 Pearson: 0.23817939952805853 MAE: 0.6982953214505471\n",
      "Epoch: 13 RMSE: 0.8829247586896741 Pearson: 0.2449184511167223 MAE: 0.6990251200681787\n",
      "Epoch: 14 RMSE: 0.8802504889487123 Pearson: 0.25345435380994563 MAE: 0.704740716493405\n",
      "Epoch: 15 RMSE: 0.8793602250257055 Pearson: 0.2621863703465891 MAE: 0.6898022036703849\n",
      "Epoch: 16 RMSE: 0.8766522116963007 Pearson: 0.27036243781354957 MAE: 0.690719937747935\n",
      "Epoch: 17 RMSE: 0.8765186895241849 Pearson: 0.27888906894258886 MAE: 0.6812511516293558\n",
      "Epoch: 18 RMSE: 0.8732635556942945 Pearson: 0.286150187199253 MAE: 0.6833373697252023\n",
      "Epoch: 19 RMSE: 0.8699672021871135 Pearson: 0.29357821354751285 MAE: 0.6989058222564261\n",
      "Epoch: 20 RMSE: 0.8698102564933828 Pearson: 0.30012076128999515 MAE: 0.6790685611890331\n",
      "Epoch: 21 RMSE: 0.8665725148917113 Pearson: 0.3058685130900879 MAE: 0.6863080375318573\n",
      "Epoch: 22 RMSE: 0.8650103050046359 Pearson: 0.31084969522645345 MAE: 0.694636660541473\n",
      "Epoch: 23 RMSE: 0.8735143943320566 Pearson: 0.3136244644274196 MAE: 0.6630883431512573\n",
      "Epoch: 24 RMSE: 0.8633425491244592 Pearson: 0.3187258712006106 MAE: 0.6781023760815144\n",
      "Epoch: 25 RMSE: 0.8614441214705708 Pearson: 0.32276757340611223 MAE: 0.6933345453044157\n",
      "Epoch: 26 RMSE: 0.8659771395341358 Pearson: 0.32347627527456313 MAE: 0.6657144118383871\n",
      "Epoch: 27 RMSE: 0.859840116378051 Pearson: 0.32718491345997586 MAE: 0.679220473250871\n",
      "Epoch: 28 RMSE: 0.8587993705222622 Pearson: 0.33095672582293173 MAE: 0.6920637431917064\n",
      "Epoch: 29 RMSE: 0.8604839298374302 Pearson: 0.3308732236434489 MAE: 0.6703610482307549\n",
      "Epoch: 30 RMSE: 0.8584595829051848 Pearson: 0.33517571917433914 MAE: 0.6717570202981965\n",
      "Epoch: 31 RMSE: 0.857629421182806 Pearson: 0.3374604989346715 MAE: 0.697730150693493\n",
      "Epoch: 32 RMSE: 0.8550529050171904 Pearson: 0.3410655374163907 MAE: 0.6850205991193967\n",
      "Epoch: 33 RMSE: 0.8579495118633115 Pearson: 0.3412139101203585 MAE: 0.6664333569109244\n",
      "Epoch: 34 RMSE: 0.8548988121797867 Pearson: 0.34107564754520026 MAE: 0.6786807793256392\n",
      "Epoch: 35 RMSE: 0.8540453738183572 Pearson: 0.3441635654866733 MAE: 0.6758165912703059\n",
      "Epoch: 36 RMSE: 0.8530568545728214 Pearson: 0.3463850105307742 MAE: 0.6795797555010302\n",
      "Epoch: 37 RMSE: 0.8527656132234415 Pearson: 0.3473748312604313 MAE: 0.6832913023653368\n",
      "Epoch: 38 RMSE: 0.858709108258681 Pearson: 0.34716949562059995 MAE: 0.7104056927737907\n",
      "Epoch: 39 RMSE: 0.8521364548066027 Pearson: 0.35008681584613016 MAE: 0.6744508993039442\n",
      "Epoch: 40 RMSE: 0.8507341278249937 Pearson: 0.35327107591478607 MAE: 0.6805086462079207\n",
      "Epoch: 41 RMSE: 0.8590045357099747 Pearson: 0.35252246017738303 MAE: 0.6579765391382711\n",
      "Epoch: 42 RMSE: 0.8513183439355342 Pearson: 0.35397861845954687 MAE: 0.6885880600246282\n",
      "Epoch: 43 RMSE: 0.8506377350400479 Pearson: 0.35800027703138326 MAE: 0.6665435100097057\n",
      "Epoch: 44 RMSE: 0.8496916306548727 Pearson: 0.35772380668287584 MAE: 0.6855534559687808\n",
      "Epoch: 45 RMSE: 0.879136368019936 Pearson: 0.35280203741421134 MAE: 0.749521410514578\n",
      "Epoch: 46 RMSE: 0.8512493522608893 Pearson: 0.3589255998835306 MAE: 0.6935679535481093\n",
      "Epoch: 47 RMSE: 0.8587342956497993 Pearson: 0.3550100081838163 MAE: 0.7086066040290455\n",
      "Epoch: 48 RMSE: 0.849895424971075 Pearson: 0.3573738614610393 MAE: 0.6717347066094183\n",
      "Epoch: 49 RMSE: 0.8546114359456016 Pearson: 0.3578381808242496 MAE: 0.6626289287758408\n",
      "Epoch: 50 RMSE: 0.8514674018472868 Pearson: 0.36026270296502844 MAE: 0.6909797385386396\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ8AAAEWCAYAAAC5XZqEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats.stats import pearsonr\n",
    "import random\n",
    "\n",
    "net = Net(n_feature=200, n_output=1)     # define the network\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "loss_func = torch.nn.MSELoss()  \n",
    "batch_size = 64\n",
    "epoch = 50\n",
    "losses = []\n",
    "batch_idx = np.array(list(range(len(X_train))))\n",
    "\n",
    "for e in range(1, epoch+1):\n",
    "    random.shuffle(batch_idx)\n",
    "    for i in range(0, len(X_train)-batch_size, batch_size):\n",
    "        curr_idx = batch_idx[i:i+batch_size]\n",
    "        x = torch.Tensor(X_train[curr_idx])\n",
    "        y = torch.Tensor(y_train_zh[curr_idx]).view(batch_size,-1)\n",
    "        optimizer.zero_grad()\n",
    "        prediction = net(x)    \n",
    "        loss = loss_func(prediction, y)\n",
    "        losses += [loss.item()]\n",
    "        optimizer.zero_grad()   \n",
    "        loss.backward()        \n",
    "        optimizer.step()    \n",
    "    with torch.no_grad():\n",
    "        predictions = net(torch.Tensor(X_val)).flatten().data.numpy()\n",
    "        pearson = pearsonr(y_val_zh, predictions)\n",
    "        print(f'Epoch: {e} RMSE: {rmse(predictions,y_val_zh)} Pearson: {pearson[0]} MAE: {mean_absolute_error(predictions,y_val_zh)}')\n",
    "plot_loss(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_train_mt_m2 = get_sentence_embeddings_zh(\"./train.enzh.mt\", mode=2)\n",
    "zh_train_src_m2 = get_embeddings(\"./train.enzh.src\", glove, nlp_en, mode=2)\n",
    "\n",
    "zh_val_src_m2 = get_embeddings(\"./dev.enzh.src\", glove, nlp_en, mode=2)\n",
    "zh_val_mt_m2 = get_sentence_embeddings_zh(\"./dev.enzh.mt\", mode=2)\n",
    "\n",
    "zh_test_src_m2 = get_embeddings(\"./test.enzh.src\", glove, nlp_en, mode=2)\n",
    "zh_test_mt_m2 = get_sentence_embeddings_zh(\"./test.enzh.mt\", mode=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_max_length(sentences, max_len):\n",
    "    return torch.stack(tuple(\n",
    "                        F.pad(input=torch.tensor(l), pad=(0, 0, 0, max_len - l.shape[0]), mode='constant', value=0) \n",
    "                        for l in sentences))\n",
    "\n",
    "max_len_zh_1 = max(l.shape[0] for l in zh_train_mt_m2)\n",
    "max_len_en_1 = max(l.shape[0] for l in zh_train_src_m2)\n",
    "max_len_zh_2 = max(l.shape[0] for l in zh_val_mt_m2)\n",
    "max_len_en_2 = max(l.shape[0] for l in zh_val_src_m2)\n",
    "max_len_zh_3 = max(l.shape[0] for l in zh_test_mt_m2)\n",
    "max_len_en_3 = max(l.shape[0] for l in zh_test_src_m2)\n",
    "max_len_zh = max(max_len_zh_1, max_len_zh_2, max_len_zh_3)\n",
    "max_len_en = max(max_len_en_1, max_len_en_2, max_len_en_3)\n",
    "\n",
    "zh_train_mt_pad = pad_to_max_length(zh_train_mt_m2, max_len_zh)\n",
    "zh_train_src_pad = pad_to_max_length(zh_train_src_m2, max_len_en)\n",
    "zh_val_src_pad = pad_to_max_length(zh_val_src_m2, max_len_en)\n",
    "zh_val_mt_pad = pad_to_max_length(zh_val_mt_m2, max_len_zh)\n",
    "zh_test_src_pad = pad_to_max_length(zh_test_src_m2, max_len_en)\n",
    "zh_test_mt_pad = pad_to_max_length(zh_test_mt_m2, max_len_zh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRUNet(\n",
       "  (gru_en): GRU(100, 50, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (gru_zh): GRU(100, 50, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (fc2): Linear(in_features=50, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=16, bias=True)\n",
       "  (fc4): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = word_dim\n",
    "hidden_dim=50\n",
    "\n",
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, fc_dim, n_layers, drop_prob=0, bidirectional=True):\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.gru_en = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob, bidirectional=bidirectional).to(device)\n",
    "        self.gru_zh = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob, bidirectional=bidirectional).to(device)\n",
    "        self.fc2 = nn.Linear(fc_dim, 64).to(device)\n",
    "        self.fc3 = nn.Linear(64, 16).to(device)\n",
    "        self.fc4 = nn.Linear(16, output_dim).to(device)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        x_en, x_zh = x[0].to(device), x[1].to(device)\n",
    "        h_en, h_zh = h[0].to(device), h[1].to(device)\n",
    "        out_en, h_en = self.gru_en(x_en, h_en)\n",
    "        out_zh, h_zh = self.gru_zh(x_zh, h_zh)\n",
    "        out = torch.cat((torch.mean(out_en, 1),torch.mean(out_zh, 1)), 1).view(batch_size, -1)\n",
    "        out = self.relu(self.fc2(out))\n",
    "        out = self.relu(self.fc3(out))\n",
    "        out = self.fc4(out)\n",
    "        return out, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(self.n_layers*2, batch_size, self.hidden_dim).zero_().to(device)\n",
    "        return hidden\n",
    "\n",
    "GRUNet(input_dim, hidden_dim, fc_dim=(1)*hidden_dim, output_dim=1, n_layers=2, drop_prob=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 RMSE: 0.8972307 Pearson 0.0183719 MAE: 0.7265106\n",
      "Epoch: 2 RMSE: 0.8972595 Pearson 0.0392013 MAE: 0.7223803\n",
      "Epoch: 3 RMSE: 0.8976585 Pearson 0.0600813 MAE: 0.7171305\n",
      "Epoch: 4 RMSE: 0.8972899 Pearson 0.0740922 MAE: 0.7202278\n",
      "Epoch: 5 RMSE: 0.8971270 Pearson 0.0894106 MAE: 0.7218133\n",
      "Epoch: 6 RMSE: 0.8971487 Pearson 0.1048902 MAE: 0.7204615\n",
      "Epoch: 7 RMSE: 0.8970167 Pearson 0.1177385 MAE: 0.7215109\n",
      "Epoch: 8 RMSE: 0.8970910 Pearson 0.1293521 MAE: 0.7195462\n",
      "Epoch: 9 RMSE: 0.8970872 Pearson 0.1384882 MAE: 0.7188123\n",
      "Epoch: 10 RMSE: 0.8972029 Pearson 0.1470660 MAE: 0.7170626\n",
      "Epoch: 11 RMSE: 0.8968270 Pearson 0.1517459 MAE: 0.7200965\n",
      "Epoch: 12 RMSE: 0.8970135 Pearson 0.1585963 MAE: 0.7172105\n",
      "Epoch: 13 RMSE: 0.8967938 Pearson 0.1626093 MAE: 0.7183521\n",
      "Epoch: 14 RMSE: 0.8964280 Pearson 0.1601265 MAE: 0.7227643\n",
      "Epoch: 15 RMSE: 0.8966165 Pearson 0.1651108 MAE: 0.7179384\n",
      "Epoch: 16 RMSE: 0.8961828 Pearson 0.1675312 MAE: 0.7217065\n",
      "Epoch: 17 RMSE: 0.8960087 Pearson 0.1709995 MAE: 0.7214937\n",
      "Epoch: 18 RMSE: 0.8965266 Pearson 0.1759491 MAE: 0.7142093\n",
      "Epoch: 19 RMSE: 0.8956686 Pearson 0.1747534 MAE: 0.7199724\n",
      "Epoch: 20 RMSE: 0.8953053 Pearson 0.1768581 MAE: 0.7222826\n",
      "Epoch: 21 RMSE: 0.8950807 Pearson 0.1803363 MAE: 0.7201042\n",
      "Epoch: 22 RMSE: 0.8951359 Pearson 0.1840585 MAE: 0.7151808\n",
      "Epoch: 23 RMSE: 0.8945194 Pearson 0.1862216 MAE: 0.7168683\n",
      "Epoch: 24 RMSE: 0.8940839 Pearson 0.1887504 MAE: 0.7162063\n",
      "Epoch: 25 RMSE: 0.8934262 Pearson 0.1910683 MAE: 0.7172362\n",
      "Epoch: 26 RMSE: 0.8926735 Pearson 0.1933514 MAE: 0.7192613\n",
      "Epoch: 27 RMSE: 0.8920621 Pearson 0.1950117 MAE: 0.7248939\n",
      "Epoch: 28 RMSE: 0.8912809 Pearson 0.1971938 MAE: 0.7151167\n",
      "Epoch: 29 RMSE: 0.8909221 Pearson 0.1989730 MAE: 0.7091483\n",
      "Epoch: 30 RMSE: 0.8890861 Pearson 0.2005282 MAE: 0.7167212\n",
      "Epoch: 31 RMSE: 0.8886372 Pearson 0.2024993 MAE: 0.7063622\n",
      "Epoch: 32 RMSE: 0.8865081 Pearson 0.2041252 MAE: 0.7113348\n",
      "Epoch: 33 RMSE: 0.8851879 Pearson 0.2062715 MAE: 0.7067674\n",
      "Epoch: 34 RMSE: 0.8832072 Pearson 0.2078158 MAE: 0.7130861\n",
      "Epoch: 35 RMSE: 0.8815516 Pearson 0.2107398 MAE: 0.7133722\n",
      "Epoch: 36 RMSE: 0.8799140 Pearson 0.2138959 MAE: 0.7037183\n",
      "Epoch: 37 RMSE: 0.8791673 Pearson 0.2173393 MAE: 0.6947833\n",
      "Epoch: 38 RMSE: 0.8775798 Pearson 0.2208595 MAE: 0.6938022\n",
      "Epoch: 39 RMSE: 0.8756278 Pearson 0.2250520 MAE: 0.7095928\n",
      "Epoch: 40 RMSE: 0.8759290 Pearson 0.2284206 MAE: 0.6865981\n",
      "Epoch: 41 RMSE: 0.8730839 Pearson 0.2331003 MAE: 0.6984203\n",
      "Epoch: 42 RMSE: 0.8721135 Pearson 0.2374076 MAE: 0.6947682\n",
      "Epoch: 43 RMSE: 0.8720697 Pearson 0.2413355 MAE: 0.6857978\n",
      "Epoch: 44 RMSE: 0.8700501 Pearson 0.2456098 MAE: 0.6971560\n",
      "Epoch: 45 RMSE: 0.8694829 Pearson 0.2492757 MAE: 0.7022861\n",
      "Epoch: 46 RMSE: 0.8698175 Pearson 0.2523873 MAE: 0.6812859\n",
      "Epoch: 47 RMSE: 0.8684636 Pearson 0.2560913 MAE: 0.6826947\n",
      "Epoch: 48 RMSE: 0.8676186 Pearson 0.2593062 MAE: 0.6821765\n",
      "Epoch: 49 RMSE: 0.8660027 Pearson 0.2624154 MAE: 0.6929109\n",
      "Epoch: 50 RMSE: 0.8671326 Pearson 0.2652436 MAE: 0.6765251\n",
      "Epoch: 51 RMSE: 0.8647149 Pearson 0.2676556 MAE: 0.6891928\n",
      "Epoch: 52 RMSE: 0.8653952 Pearson 0.2701087 MAE: 0.6772390\n",
      "Epoch: 53 RMSE: 0.8642388 Pearson 0.2720878 MAE: 0.6799896\n",
      "Epoch: 54 RMSE: 0.8634911 Pearson 0.2737367 MAE: 0.6968935\n",
      "Epoch: 55 RMSE: 0.8652413 Pearson 0.2756056 MAE: 0.6713830\n",
      "Epoch: 56 RMSE: 0.8621104 Pearson 0.2781126 MAE: 0.6834376\n",
      "Epoch: 57 RMSE: 0.8619645 Pearson 0.2793499 MAE: 0.6949326\n",
      "Epoch: 58 RMSE: 0.8648880 Pearson 0.2798894 MAE: 0.7122433\n",
      "Epoch: 59 RMSE: 0.8609043 Pearson 0.2826379 MAE: 0.6918798\n",
      "Epoch: 60 RMSE: 0.8612031 Pearson 0.2856303 MAE: 0.6737961\n",
      "Epoch: 61 RMSE: 0.8608546 Pearson 0.2852931 MAE: 0.6975708\n",
      "Epoch: 62 RMSE: 0.8595808 Pearson 0.2870730 MAE: 0.6827930\n",
      "Epoch: 63 RMSE: 0.8592173 Pearson 0.2886977 MAE: 0.6810861\n",
      "Epoch: 64 RMSE: 0.8589973 Pearson 0.2903154 MAE: 0.6780248\n",
      "Epoch: 65 RMSE: 0.8586325 Pearson 0.2907230 MAE: 0.6804565\n",
      "Epoch: 66 RMSE: 0.8581933 Pearson 0.2924376 MAE: 0.6882398\n",
      "Epoch: 67 RMSE: 0.8593722 Pearson 0.2943650 MAE: 0.6686218\n",
      "Epoch: 68 RMSE: 0.8585780 Pearson 0.2953852 MAE: 0.6703374\n",
      "Epoch: 69 RMSE: 0.8572323 Pearson 0.2959791 MAE: 0.6782986\n",
      "Epoch: 70 RMSE: 0.8570971 Pearson 0.2963960 MAE: 0.6868536\n",
      "Epoch: 71 RMSE: 0.8567506 Pearson 0.2983936 MAE: 0.6752322\n",
      "Epoch: 72 RMSE: 0.8563114 Pearson 0.2987716 MAE: 0.6798657\n",
      "Epoch: 73 RMSE: 0.8563933 Pearson 0.3007525 MAE: 0.6718109\n",
      "Epoch: 74 RMSE: 0.8562233 Pearson 0.3003595 MAE: 0.6880630\n",
      "Epoch: 75 RMSE: 0.8561986 Pearson 0.3046882 MAE: 0.6663340\n",
      "Epoch: 76 RMSE: 0.8548188 Pearson 0.3042013 MAE: 0.6768962\n",
      "Epoch: 77 RMSE: 0.8554243 Pearson 0.3051357 MAE: 0.6915962\n",
      "Epoch: 78 RMSE: 0.8537769 Pearson 0.3077385 MAE: 0.6763334\n",
      "Epoch: 79 RMSE: 0.8554687 Pearson 0.3092733 MAE: 0.6624014\n",
      "Epoch: 80 RMSE: 0.8548232 Pearson 0.3080591 MAE: 0.6923259\n",
      "Epoch: 81 RMSE: 0.8532900 Pearson 0.3109905 MAE: 0.6864929\n",
      "Epoch: 82 RMSE: 0.8590569 Pearson 0.3112502 MAE: 0.6536000\n",
      "Epoch: 83 RMSE: 0.8548729 Pearson 0.3119056 MAE: 0.6962949\n",
      "Epoch: 84 RMSE: 0.8598881 Pearson 0.3147052 MAE: 0.6506583\n",
      "Epoch: 85 RMSE: 0.8546296 Pearson 0.3161157 MAE: 0.6573065\n",
      "Epoch: 86 RMSE: 0.8513364 Pearson 0.3167246 MAE: 0.6813732\n",
      "Epoch: 87 RMSE: 0.8539688 Pearson 0.3170246 MAE: 0.6979936\n",
      "Epoch: 88 RMSE: 0.8504189 Pearson 0.3189681 MAE: 0.6741486\n",
      "Epoch: 89 RMSE: 0.8514272 Pearson 0.3189083 MAE: 0.6640082\n",
      "Epoch: 90 RMSE: 0.8506970 Pearson 0.3184221 MAE: 0.6712466\n",
      "Epoch: 91 RMSE: 0.8543784 Pearson 0.3188543 MAE: 0.6990012\n",
      "Epoch: 92 RMSE: 0.8507997 Pearson 0.3212176 MAE: 0.6627558\n",
      "Epoch: 93 RMSE: 0.8564211 Pearson 0.3217354 MAE: 0.6500617\n",
      "Epoch: 94 RMSE: 0.8520440 Pearson 0.3240610 MAE: 0.6552267\n",
      "Epoch: 95 RMSE: 0.8527997 Pearson 0.3214271 MAE: 0.6554686\n",
      "Epoch: 96 RMSE: 0.8508353 Pearson 0.3209118 MAE: 0.6635202\n",
      "Epoch: 97 RMSE: 0.8499535 Pearson 0.3249015 MAE: 0.6603798\n",
      "Epoch: 98 RMSE: 0.8491371 Pearson 0.3238856 MAE: 0.6737886\n",
      "Epoch: 99 RMSE: 0.8491248 Pearson 0.3268004 MAE: 0.6854392\n",
      "Epoch: 100 RMSE: 0.8496501 Pearson 0.3265683 MAE: 0.6847366\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ8AAAEWCAYAAAC5XZqEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats.stats import pearsonr\n",
    "import random\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "lr=0.01\n",
    "batch_size = 64\n",
    "epoch = 100\n",
    "losses = []\n",
    "\n",
    "X_en = zh_train_src_pad\n",
    "X_zh = zh_train_mt_pad\n",
    "\n",
    "gru = GRUNet(input_dim, hidden_dim, fc_dim=4*hidden_dim, output_dim=1, n_layers=2, drop_prob=0)\n",
    "optimizer = torch.optim.SGD(gru.parameters(), lr)\n",
    "loss_func = torch.nn.MSELoss() # lambda x, y : torch.nn.L1Loss()(x, y) + torch.nn.MSELoss()(x, y) + pearsonr_loss(x.flatten(), y.flatten())\n",
    "\n",
    "h = gru.init_hidden(batch_size), gru.init_hidden(batch_size)\n",
    "batch_idx = np.array(list(range(len(X_en))))\n",
    "                     \n",
    "for e in range(1, epoch+1):\n",
    "    random.shuffle(batch_idx)\n",
    "    for i in range(0, len(X_en)-batch_size, batch_size):\n",
    "        curr_idx = batch_idx[i:i+batch_size]\n",
    "        x = X_en[curr_idx], X_zh[curr_idx]\n",
    "        y = torch.Tensor(y_train_zh[curr_idx]).view(batch_size,-1).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        prediction, h = gru(x, h)\n",
    "        loss = loss_func(prediction, y)  \n",
    "        losses += [loss.item()]\n",
    "        optimizer.zero_grad()   \n",
    "        loss.backward()        \n",
    "        optimizer.step()    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predictions = torch.zeros(0, batch_size)\n",
    "        for i in range(0, len(zh_val_src_pad)-batch_size, batch_size):\n",
    "            pred, _ = gru((zh_val_src_pad[i: i+batch_size], zh_val_mt_pad[i:i+batch_size]), h)\n",
    "            pred = pred.permute(1,0)\n",
    "            predictions = torch.cat((predictions, pred.cpu()), 0)\n",
    "        predictions = predictions.flatten()\n",
    "        y_val_trunc = torch.Tensor(y_val_zh[:len(predictions)-len(predictions)%batch_size])\n",
    "        pearson = pearsonr(y_val_trunc, predictions)\n",
    "        \n",
    "        print(f'Epoch: {e} RMSE: {rmse(y_val_trunc, predictions):.7f} Pearson {pearson[0]:.7f} MAE: {mean_absolute_error(y_val_trunc, predictions):.7f}')\n",
    "plot_loss(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating training and validation set\n",
    "\n",
    "zh_total_src = zh_train_src_m2 + zh_val_src_m2\n",
    "zh_total_mt = zh_train_mt_m2 + zh_val_mt_m2\n",
    "zh_total_mt_pad = pad_to_max_length(zh_total_mt, max_len_zh)\n",
    "zh_total_src_pad = pad_to_max_length(zh_total_src, max_len_en)\n",
    "\n",
    "y_total_zh = np.append(y_train_zh,y_val_zh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-f96f1346461c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch: {e} loss: {np.mean(losses[-len(X_en)-len(X_en)%batch_size:]):.7f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training best model on full training + validation dataset\n",
    "\n",
    "losses = []\n",
    "X_en = zh_total_src_pad\n",
    "X_zh = zh_total_mt_pad\n",
    "\n",
    "gru = GRUNet(input_dim, hidden_dim, fc_dim=4*hidden_dim, output_dim=1, n_layers=2, drop_prob=0)\n",
    "optimizer = torch.optim.SGD(gru.parameters(), lr)\n",
    "loss_func = torch.nn.MSELoss() # lambda x, y : torch.nn.L1Loss()(x, y) + torch.nn.MSELoss()(x, y) + pearsonr_loss(x.flatten(), y.flatten())\n",
    "\n",
    "h = gru.init_hidden(batch_size), gru.init_hidden(batch_size)\n",
    "batch_idx = np.array(list(range(len(X_en))))\n",
    "                     \n",
    "for e in range(1, epoch+1):\n",
    "    random.shuffle(batch_idx)\n",
    "    for i in range(0, len(X_en)-batch_size, batch_size):\n",
    "        curr_idx = batch_idx[i:i+batch_size]\n",
    "        x = X_en[curr_idx], X_zh[curr_idx]\n",
    "        y = torch.Tensor(y_total_zh[curr_idx]).view(batch_size,-1).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        prediction, h = gru(x, h)\n",
    "        loss = loss_func(prediction, y)  \n",
    "        losses += [loss.item()]\n",
    "        optimizer.zero_grad()   \n",
    "        loss.backward()        \n",
    "        optimizer.step()    \n",
    "    print(f'Epoch: {e} loss: {np.mean(losses[-len(X_en)-len(X_en)%batch_size:]):.7f}')\n",
    "plot_loss(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gru.state_dict(), \"modelGRU.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = torch.zeros(0, batch_size)\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(zh_test_src_pad)-batch_size, batch_size):\n",
    "        pred, _ = gru((zh_test_src_pad[i: i+batch_size], zh_test_mt_pad[i:i+batch_size]), h)\n",
    "        pred = pred.permute(1,0)\n",
    "        predictions = torch.cat((predictions, pred.cpu()), 0)\n",
    "    predictions = predictions.flatten()\n",
    "    # Adding the predictions for the last entries not fitting in a batch\n",
    "    predictions = torch.cat((predictions, gru((\n",
    "                        zh_test_src_pad[len(zh_test_src_pad)-batch_size: len(zh_test_src_pad)], \n",
    "                        zh_test_mt_pad[len(zh_test_src_pad)-batch_size: len(zh_test_src_pad)]\n",
    "                    ), h)[0][batch_size-len(zh_test_src_pad)%batch_size:].cpu().flatten()))\n",
    "    predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def writeScores(scores):\n",
    "    fn = \"predictions.txt\"\n",
    "    print(\"\")\n",
    "    with open(fn, 'w') as output_file:\n",
    "        for idx,x in enumerate(scores):\n",
    "            output_file.write(f\"{x}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "writeScores(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, fc_dim, n_layers, drop_prob=0):\n",
    "        super(LSTMNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.lstm_en = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob).to(device)\n",
    "        self.lstm_zh = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob).to(device)\n",
    "        self.fc = nn.Linear(fc_dim, output_dim).to(device)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        x_en, x_zh = x[0].to(device), x[1].to(device)\n",
    "        h_en, h_zh = h[0].to(device), h[1].to(device)\n",
    "        c_0 = torch.zeros([1,64,50], dtype=torch.int32)\n",
    "        out_en, h_en = self.lstm_en(x_en, (h_en, c_0))\n",
    "        out_zh, h_zh = self.lstm_zh(x_zh, (h_zh, c_0))\n",
    "        out = torch.cat((out_en, out_zh), 1)\n",
    "        out = torch.mean(out, 1)\n",
    "        out = self.fc(out)\n",
    "        return out, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 RMSE: 0.8978609 Pearson -0.0340535 MAE: 0.7208775\n",
      "Epoch: 2 RMSE: 0.8976474 Pearson -0.0191206 MAE: 0.7218169\n",
      "Epoch: 3 RMSE: 0.8973805 Pearson -0.0049471 MAE: 0.7252970\n",
      "Epoch: 4 RMSE: 0.8974202 Pearson 0.0095714 MAE: 0.7208392\n",
      "Epoch: 5 RMSE: 0.8973354 Pearson 0.0229218 MAE: 0.7200329\n",
      "Epoch: 6 RMSE: 0.8970430 Pearson 0.0351025 MAE: 0.7223168\n",
      "Epoch: 7 RMSE: 0.8970442 Pearson 0.0466874 MAE: 0.7200035\n",
      "Epoch: 8 RMSE: 0.8968695 Pearson 0.0573689 MAE: 0.7203456\n",
      "Epoch: 9 RMSE: 0.8970081 Pearson 0.0671492 MAE: 0.7173018\n",
      "Epoch: 10 RMSE: 0.8965740 Pearson 0.0760277 MAE: 0.7203380\n",
      "Epoch: 11 RMSE: 0.8963555 Pearson 0.0842173 MAE: 0.7215024\n",
      "Epoch: 12 RMSE: 0.8962941 Pearson 0.0917169 MAE: 0.7199892\n",
      "Epoch: 13 RMSE: 0.8960246 Pearson 0.0985573 MAE: 0.7221801\n",
      "Epoch: 14 RMSE: 0.8959358 Pearson 0.1047279 MAE: 0.7207707\n",
      "Epoch: 15 RMSE: 0.8957667 Pearson 0.1104343 MAE: 0.7209978\n",
      "Epoch: 16 RMSE: 0.8956715 Pearson 0.1156152 MAE: 0.7198392\n",
      "Epoch: 17 RMSE: 0.8953668 Pearson 0.1205789 MAE: 0.7255300\n",
      "Epoch: 18 RMSE: 0.8952167 Pearson 0.1248607 MAE: 0.7229547\n",
      "Epoch: 19 RMSE: 0.8950750 Pearson 0.1288502 MAE: 0.7219735\n",
      "Epoch: 20 RMSE: 0.8952527 Pearson 0.1323450 MAE: 0.7166340\n",
      "Epoch: 21 RMSE: 0.8947288 Pearson 0.1359738 MAE: 0.7219446\n",
      "Epoch: 22 RMSE: 0.8949108 Pearson 0.1389214 MAE: 0.7163166\n",
      "Epoch: 23 RMSE: 0.8944244 Pearson 0.1420228 MAE: 0.7202902\n",
      "Epoch: 24 RMSE: 0.8945222 Pearson 0.1446160 MAE: 0.7162696\n",
      "Epoch: 25 RMSE: 0.8943116 Pearson 0.1471854 MAE: 0.7163358\n",
      "Epoch: 26 RMSE: 0.8938915 Pearson 0.1497092 MAE: 0.7191607\n",
      "Epoch: 27 RMSE: 0.8938910 Pearson 0.1518553 MAE: 0.7163362\n",
      "Epoch: 28 RMSE: 0.8934910 Pearson 0.1540933 MAE: 0.7188346\n",
      "Epoch: 29 RMSE: 0.8932065 Pearson 0.1561602 MAE: 0.7204372\n",
      "Epoch: 30 RMSE: 0.8930545 Pearson 0.1579984 MAE: 0.7187303\n",
      "Epoch: 31 RMSE: 0.8927618 Pearson 0.1598524 MAE: 0.7203206\n",
      "Epoch: 32 RMSE: 0.8926324 Pearson 0.1614742 MAE: 0.7178762\n",
      "Epoch: 33 RMSE: 0.8923172 Pearson 0.1631378 MAE: 0.7194425\n",
      "Epoch: 34 RMSE: 0.8920678 Pearson 0.1646909 MAE: 0.7195516\n",
      "Epoch: 35 RMSE: 0.8918316 Pearson 0.1661423 MAE: 0.7190717\n",
      "Epoch: 36 RMSE: 0.8917577 Pearson 0.1674683 MAE: 0.7156914\n",
      "Epoch: 37 RMSE: 0.8915668 Pearson 0.1687960 MAE: 0.7146567\n",
      "Epoch: 38 RMSE: 0.8910168 Pearson 0.1703433 MAE: 0.7220759\n",
      "Epoch: 39 RMSE: 0.8907946 Pearson 0.1715097 MAE: 0.7179289\n",
      "Epoch: 40 RMSE: 0.8905649 Pearson 0.1727283 MAE: 0.7166195\n",
      "Epoch: 41 RMSE: 0.8902821 Pearson 0.1739264 MAE: 0.7163418\n",
      "Epoch: 42 RMSE: 0.8899126 Pearson 0.1751429 MAE: 0.7182122\n",
      "Epoch: 43 RMSE: 0.8896103 Pearson 0.1762691 MAE: 0.7182637\n",
      "Epoch: 44 RMSE: 0.8892922 Pearson 0.1774209 MAE: 0.7201115\n",
      "Epoch: 45 RMSE: 0.8891265 Pearson 0.1783678 MAE: 0.7146422\n",
      "Epoch: 46 RMSE: 0.8889378 Pearson 0.1793902 MAE: 0.7126986\n",
      "Epoch: 47 RMSE: 0.8884263 Pearson 0.1804867 MAE: 0.7153648\n",
      "Epoch: 48 RMSE: 0.8880485 Pearson 0.1815567 MAE: 0.7166964\n",
      "Epoch: 49 RMSE: 0.8877627 Pearson 0.1825421 MAE: 0.7150794\n",
      "Epoch: 50 RMSE: 0.8874201 Pearson 0.1835315 MAE: 0.7149423\n",
      "Epoch: 51 RMSE: 0.8872156 Pearson 0.1844798 MAE: 0.7121984\n",
      "Epoch: 52 RMSE: 0.8866957 Pearson 0.1855302 MAE: 0.7160010\n",
      "Epoch: 53 RMSE: 0.8868002 Pearson 0.1863930 MAE: 0.7086594\n",
      "Epoch: 54 RMSE: 0.8860338 Pearson 0.1874634 MAE: 0.7140579\n",
      "Epoch: 55 RMSE: 0.8861591 Pearson 0.1883281 MAE: 0.7074831\n",
      "Epoch: 56 RMSE: 0.8853098 Pearson 0.1893852 MAE: 0.7138690\n",
      "Epoch: 57 RMSE: 0.8849907 Pearson 0.1903952 MAE: 0.7193440\n",
      "Epoch: 58 RMSE: 0.8846837 Pearson 0.1912632 MAE: 0.7110614\n",
      "Epoch: 59 RMSE: 0.8842546 Pearson 0.1922902 MAE: 0.7122723\n",
      "Epoch: 60 RMSE: 0.8841072 Pearson 0.1932442 MAE: 0.7083705\n",
      "Epoch: 61 RMSE: 0.8836985 Pearson 0.1942396 MAE: 0.7086678\n",
      "Epoch: 62 RMSE: 0.8831501 Pearson 0.1953225 MAE: 0.7159749\n",
      "Epoch: 63 RMSE: 0.8830132 Pearson 0.1962582 MAE: 0.7075351\n",
      "Epoch: 64 RMSE: 0.8824673 Pearson 0.1972988 MAE: 0.7104892\n",
      "Epoch: 65 RMSE: 0.8830052 Pearson 0.1982658 MAE: 0.7011172\n",
      "Epoch: 66 RMSE: 0.8817300 Pearson 0.1993515 MAE: 0.7109078\n",
      "Epoch: 67 RMSE: 0.8814275 Pearson 0.2003912 MAE: 0.7089611\n",
      "Epoch: 68 RMSE: 0.8810417 Pearson 0.2014488 MAE: 0.7098253\n",
      "Epoch: 69 RMSE: 0.8807482 Pearson 0.2025068 MAE: 0.7079374\n",
      "Epoch: 70 RMSE: 0.8807682 Pearson 0.2035943 MAE: 0.7028599\n",
      "Epoch: 71 RMSE: 0.8804621 Pearson 0.2046584 MAE: 0.7021816\n",
      "Epoch: 72 RMSE: 0.8797240 Pearson 0.2058028 MAE: 0.7071618\n",
      "Epoch: 73 RMSE: 0.8793371 Pearson 0.2069201 MAE: 0.7105551\n",
      "Epoch: 74 RMSE: 0.8796078 Pearson 0.2080646 MAE: 0.6999550\n",
      "Epoch: 75 RMSE: 0.8787706 Pearson 0.2091815 MAE: 0.7128210\n",
      "Epoch: 76 RMSE: 0.8796095 Pearson 0.2104176 MAE: 0.6954559\n",
      "Epoch: 77 RMSE: 0.8781349 Pearson 0.2115653 MAE: 0.7121727\n",
      "Epoch: 78 RMSE: 0.8777859 Pearson 0.2127991 MAE: 0.7046211\n",
      "Epoch: 79 RMSE: 0.8776162 Pearson 0.2140635 MAE: 0.7017139\n",
      "Epoch: 80 RMSE: 0.8770925 Pearson 0.2152687 MAE: 0.7059742\n",
      "Epoch: 81 RMSE: 0.8769357 Pearson 0.2165549 MAE: 0.7016347\n",
      "Epoch: 82 RMSE: 0.8764755 Pearson 0.2178504 MAE: 0.7045290\n",
      "Epoch: 83 RMSE: 0.8763852 Pearson 0.2191582 MAE: 0.6998934\n",
      "Epoch: 84 RMSE: 0.8760366 Pearson 0.2204559 MAE: 0.6999711\n",
      "Epoch: 85 RMSE: 0.8766703 Pearson 0.2218969 MAE: 0.6925905\n",
      "Epoch: 86 RMSE: 0.8752378 Pearson 0.2230904 MAE: 0.7070910\n",
      "Epoch: 87 RMSE: 0.8748958 Pearson 0.2244713 MAE: 0.7055911\n",
      "Epoch: 88 RMSE: 0.8751789 Pearson 0.2259394 MAE: 0.6946915\n",
      "Epoch: 89 RMSE: 0.8743137 Pearson 0.2272562 MAE: 0.7005056\n",
      "Epoch: 90 RMSE: 0.8740721 Pearson 0.2286255 MAE: 0.6987034\n",
      "Epoch: 91 RMSE: 0.8743467 Pearson 0.2300949 MAE: 0.6927271\n",
      "Epoch: 92 RMSE: 0.8738294 Pearson 0.2314378 MAE: 0.6937733\n",
      "Epoch: 93 RMSE: 0.8729846 Pearson 0.2327639 MAE: 0.7010694\n",
      "Epoch: 94 RMSE: 0.8727176 Pearson 0.2340894 MAE: 0.7042158\n",
      "Epoch: 95 RMSE: 0.8732572 Pearson 0.2356755 MAE: 0.6901156\n",
      "Epoch: 96 RMSE: 0.8721431 Pearson 0.2370113 MAE: 0.6965176\n",
      "Epoch: 97 RMSE: 0.8717430 Pearson 0.2383348 MAE: 0.7027401\n",
      "Epoch: 98 RMSE: 0.8713838 Pearson 0.2397844 MAE: 0.6985757\n",
      "Epoch: 99 RMSE: 0.8711545 Pearson 0.2411935 MAE: 0.6956960\n",
      "Epoch: 100 RMSE: 0.8709267 Pearson 0.2426133 MAE: 0.6938268\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ8AAAEWCAYAAAC5XZqEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4C+fAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats.stats import pearsonr\n",
    "import random\n",
    "\n",
    "input_dim = word_dim\n",
    "hidden_dim = 50\n",
    "lr=0.01\n",
    "batch_size = 64\n",
    "steps = 5000\n",
    "losses = []\n",
    "epochs = 100\n",
    "\n",
    "X_en = zh_train_src_pad\n",
    "X_zh = zh_train_mt_pad\n",
    "\n",
    "lstm = LSTMNet(input_dim, hidden_dim, fc_dim=(1)*hidden_dim, output_dim=1, n_layers=1)\n",
    "h = lstm.init_hidden(batch_size), lstm.init_hidden(batch_size)\n",
    "optimizer = torch.optim.SGD(lstm.parameters(), lr)\n",
    "loss_func = torch.nn.MSELoss() \n",
    "\n",
    "batch_idx = np.array(list(range(len(X_en))))\n",
    "                     \n",
    "for e in range(1, epochs+1):\n",
    "    random.shuffle(batch_idx)\n",
    "    for i in range(0, len(X_en)-batch_size, batch_size):\n",
    "        curr_idx = batch_idx[i:i+batch_size]\n",
    "        x = X_en[curr_idx], X_zh[curr_idx]\n",
    "        y = torch.Tensor(y_train_zh[curr_idx]).view(batch_size,-1).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        prediction, h = lstm(x, h)\n",
    "        loss = loss_func(prediction, y)  \n",
    "        losses += [loss.item()]\n",
    "        optimizer.zero_grad()   \n",
    "        loss.backward()        \n",
    "        optimizer.step()    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predictions = torch.zeros(0, batch_size)\n",
    "        for i in range(0, len(zh_val_src_pad)-batch_size, batch_size):\n",
    "            pred, _ = lstm((zh_val_src_pad[i: i+batch_size], zh_val_mt_pad[i:i+batch_size]), h)\n",
    "            pred = pred.permute(1,0)\n",
    "            predictions = torch.cat((predictions, pred.cpu()), 0)\n",
    "        predictions = predictions.flatten()\n",
    "        y_val_trunc = torch.Tensor(y_val_zh[:len(predictions)-len(predictions)%batch_size])\n",
    "        pearson = pearsonr(y_val_trunc, predictions)\n",
    "        \n",
    "        print(f'Epoch: {e} RMSE: {rmse(y_val_trunc, predictions):.7f} Pearson {pearson[0]:.7f} MAE: {mean_absolute_error(y_val_trunc, predictions):.7f}')\n",
    "plot_loss(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lstm.state_dict(), \"modelLSTM.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
