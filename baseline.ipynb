{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Coursework:-Baseline-Model\" data-toc-modified-id=\"Coursework:-Baseline-Model-1\">Coursework: Baseline Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#A.-English-German\" data-toc-modified-id=\"A.-English-German-1.1\">A. English-German</a></span><ul class=\"toc-item\"><li><span><a href=\"#Importing-Data\" data-toc-modified-id=\"Importing-Data-1.1.1\">Importing Data</a></span></li><li><span><a href=\"#Computing-Sentence-Embeddings\" data-toc-modified-id=\"Computing-Sentence-Embeddings-1.1.2\">Computing Sentence Embeddings</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pre-processing-with-Spacy\" data-toc-modified-id=\"Pre-processing-with-Spacy-1.1.2.1\">Pre-processing with Spacy</a></span></li><li><span><a href=\"#Getting-Training-and-Validation-Sets\" data-toc-modified-id=\"Getting-Training-and-Validation-Sets-1.1.2.2\">Getting Training and Validation Sets</a></span></li></ul></li><li><span><a href=\"#Training-the-Regressor\" data-toc-modified-id=\"Training-the-Regressor-1.1.3\">Training the Regressor</a></span><ul class=\"toc-item\"><li><span><a href=\"#SVM\" data-toc-modified-id=\"SVM-1.1.3.1\">SVM</a></span></li><li><span><a href=\"#Random-Tree-Forest\" data-toc-modified-id=\"Random-Tree-Forest-1.1.3.2\">Random Tree Forest</a></span></li></ul></li><li><span><a href=\"#Writing-Results\" data-toc-modified-id=\"Writing-Results-1.1.4\">Writing Results</a></span></li><li><span><a href=\"#Results\" data-toc-modified-id=\"Results-1.1.5\">Results</a></span></li><li><span><a href=\"#Importing-Data\" data-toc-modified-id=\"Importing-Data-1.1.6\">Importing Data</a></span></li><li><span><a href=\"#Computing-Sentence-Embeddings\" data-toc-modified-id=\"Computing-Sentence-Embeddings-1.1.7\">Computing Sentence Embeddings</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pre-processing-English-with-GloVe\" data-toc-modified-id=\"Pre-processing-English-with-GloVe-1.1.7.1\">Pre-processing English with GloVe</a></span></li><li><span><a href=\"#Loading-Chinese-Word2Vec-Embeddings\" data-toc-modified-id=\"Loading-Chinese-Word2Vec-Embeddings-1.1.7.2\">Loading Chinese Word2Vec Embeddings</a></span></li><li><span><a href=\"#Pre-processing-Chinese\" data-toc-modified-id=\"Pre-processing-Chinese-1.1.7.3\">Pre-processing Chinese</a></span></li></ul></li><li><span><a href=\"#Training-the-Regressor\" data-toc-modified-id=\"Training-the-Regressor-1.1.8\">Training the Regressor</a></span><ul class=\"toc-item\"><li><span><a href=\"#SVM\" data-toc-modified-id=\"SVM-1.1.8.1\">SVM</a></span></li><li><span><a href=\"#Random-Tree-Forest\" data-toc-modified-id=\"Random-Tree-Forest-1.1.8.2\">Random Tree Forest</a></span></li></ul></li><li><span><a href=\"#Writing-Results\" data-toc-modified-id=\"Writing-Results-1.1.9\">Writing Results</a></span></li><li><span><a href=\"#Results\" data-toc-modified-id=\"Results-1.1.10\">Results</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "omEQHVdOS61G"
   },
   "source": [
    "# Coursework: Baseline Model\n",
    "\n",
    "This notebook takes you step by step to the implementation of a simple baseline model to get you started on the coursework. You have a section for the English-German task and another for English-Chinese. They are made to be standalone so feel free to check only one of the sections. However, as the tasks require slighlty different approaches, going through both sections could help you to get inspired for your chosen task, especially each task processes english in a slighlty different way.\n",
    "\n",
    "Enjoy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lweXud1Wpemd"
   },
   "source": [
    "## A. English-German"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yu6s3YOf_C93"
   },
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "scs7ICZrPFcs"
   },
   "outputs": [],
   "source": [
    "# Download and unzip the data\n",
    "from os.path import exists\n",
    "if not exists('ende_data.zip'):\n",
    "    !wget -O ende_data.zip https://competitions.codalab.org/my/datasets/download/c748d2c0-d6be-4e36-9f12-ca0e88819c4d\n",
    "    !unzip ende_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jPy_iwHnOSAZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---EN-DE---\n",
      "\n",
      "Source:  José Ortega y Gasset visited Husserl at Freiburg in 1934.\n",
      "\n",
      "Translation:  1934 besuchte José Ortega y Gasset Husserl in Freiburg.\n",
      "\n",
      "Score:  1.1016968715664406\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the files\n",
    "import io\n",
    "\n",
    "#English-German\n",
    "print(\"---EN-DE---\")\n",
    "print()\n",
    "\n",
    "with open(\"./data/en-de/train.ende.src\", \"r\", encoding=\"utf8\") as ende_src:\n",
    "  print(\"Source: \",ende_src.readline())\n",
    "with open(\"./data/en-de/train.ende.mt\", \"r\", encoding=\"utf8\") as ende_mt:\n",
    "  print(\"Translation: \",ende_mt.readline())\n",
    "with open(\"./data/en-de/train.ende.scores\", \"r\", encoding=\"utf8\") as ende_scores:\n",
    "  print(\"Score: \",ende_scores.readline())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wiFHVnfH_Jpv"
   },
   "source": [
    "### Computing Sentence Embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g05fv5GiSyQ4"
   },
   "source": [
    "For this baseline model, we will simply use pre-trained GloVe embeddings via the Spacy module and compute the vector for each word and take the global mean for each sentence. We will do the same for both source and translation sentences. For chinese tokenization and embeddings we will have to find other tools.\n",
    "\n",
    "This is a very simplistic approach so feel free to be more creative and play around with how the sentence embeddings are computed for example ;)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jcjZpNlra8TD"
   },
   "source": [
    "GloVe embeddings do not support the Chinese language so in the section of the English-Chinese task we will have to download pretrained Chinese embeddings from word2vec repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "96bRtBbuZLJe"
   },
   "outputs": [],
   "source": [
    "#Downloading spacy models for english and german\n",
    "\n",
    "!python -m spacy download en_core_web_md\n",
    "print(\"en_core_web_md downloaded\")\n",
    "!python -m spacy link en_core_web_md en300\n",
    "print(\"en_core_web_md linked\")\n",
    "\n",
    "!python -m spacy download de_core_news_md\n",
    "print(\"de_core_news_md downloaded\")\n",
    "!python -m spacy link de_core_news_md de300\n",
    "print(\"de_core_news_md linked\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Om6kQX5bX2mB"
   },
   "source": [
    "We can now write our functions that will return the average embeddings for a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nhT2I6WYavY4"
   },
   "source": [
    "#### Pre-processing with Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "19gsNCgnW8ZT"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'AlignedSent' from 'nltk.translate' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-081da56c2bf6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdownload\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gmler\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\nltk\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslate\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msem\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[1;31m# Packages which can be lazily imported\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gmler\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\nltk\\stem\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misri\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mISRIStemmer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msnowball\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSnowballStemmer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwordnet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrslp\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRSLPStemmer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gmler\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\nltk\\stem\\snowball.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mporter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msuffix_replace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix_replace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gmler\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\nltk\\corpus\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLazyCorpusLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m abc = LazyCorpusLoader(\n",
      "\u001b[1;32mc:\\users\\gmler\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\nltk\\corpus\\reader\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchasen\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maligned\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlin\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msemcor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gmler\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\nltk\\corpus\\reader\\aligned.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWhitespaceTokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslate\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAlignedSent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAlignment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCorpusReader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'AlignedSent' from 'nltk.translate' (unknown location)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "from nltk import download\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#downloading stopwords from the nltk package\n",
    "download('stopwords') #stopwords dictionary, run once\n",
    "\n",
    "stop_words_en = set(stopwords.words('english'))\n",
    "stop_words_de = set(stopwords.words('german'))\n",
    "\n",
    "def get_sentence_emb(line,nlp,lang):\n",
    "  if lang == 'en':\n",
    "    text = line.lower()\n",
    "    l = [token.lemma_ for token in nlp.tokenizer(text)]\n",
    "    l = ' '.join([word for word in l if word not in stop_words_en])\n",
    "\n",
    "  elif lang == 'de':\n",
    "    text = line.lower()\n",
    "    l = [token.lemma_ for token in nlp.tokenizer(text)]\n",
    "    l= ' '.join([word for word in l if word not in stop_words_de])\n",
    "\n",
    "  sen = nlp(l)\n",
    "  return sen.vector\n",
    "\n",
    "def get_embeddings(f,nlp,lang):\n",
    "  file = open(f) \n",
    "  lines = file.readlines() \n",
    "  sentences_vectors =[]\n",
    "\n",
    "  for l in lines:\n",
    "      vec = get_sentence_emb(l,nlp,lang)\n",
    "      if vec is not None:\n",
    "        vec = np.mean(vec)\n",
    "        sentences_vectors.append(vec)\n",
    "      else:\n",
    "        print(\"didn't work :\", l)\n",
    "        sentences_vectors.append(0)\n",
    "\n",
    "  return sentences_vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NUKMgbo2sreI"
   },
   "source": [
    "#### Getting Training and Validation Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZXqZamIKs30T"
   },
   "source": [
    "We will now run the code fo the English-German translations and getting our training and validation sets ready for the regression task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GyJr7cIkQ3E8"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp_de = spacy.load('de300')\n",
    "nlp_en = spacy.load('en300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LwoUIDj0otbf"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "#EN-DE files\n",
    "de_train_src = get_embeddings(\"./train.ende.src\",nlp_en,'en')\n",
    "de_train_mt = get_embeddings(\"./train.ende.mt\",nlp_de,'de')\n",
    "\n",
    "f_train_scores = open(\"./train.ende.scores\",'r')\n",
    "de_train_scores = f_train_scores.readlines()\n",
    "\n",
    "de_val_src = get_embeddings(\"./dev.ende.src\",nlp_en,'en')\n",
    "de_val_mt = get_embeddings(\"./dev.ende.mt\",nlp_de,'de')\n",
    "f_val_scores = open(\"./dev.ende.scores\",'r')\n",
    "de_val_scores = f_val_scores.readlines()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U_K1CHl5VxiE"
   },
   "outputs": [],
   "source": [
    "\n",
    "#EN-DE\n",
    "print(f\"Training mt: {len(de_train_mt)} Training src: {len(de_train_src)}\")\n",
    "print()\n",
    "print(f\"Validation mt: {len(de_val_mt)} Validation src: {len(de_val_src)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Px7ikaGoy9r0"
   },
   "outputs": [],
   "source": [
    "#Put the features into a list\n",
    "X_train= [np.array(de_train_src),np.array(de_train_mt)]\n",
    "X_train_de = np.array(X_train).transpose()\n",
    "\n",
    "X_val = [np.array(de_val_src),np.array(de_val_mt)]\n",
    "X_val_de = np.array(X_val).transpose()\n",
    "\n",
    "#Scores\n",
    "train_scores = np.array(de_train_scores).astype(float)\n",
    "y_train_de =train_scores\n",
    "\n",
    "val_scores = np.array(de_val_scores).astype(float)\n",
    "y_val_de =val_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RFp6yyBl4Kgf"
   },
   "outputs": [],
   "source": [
    "\n",
    "# RUN IF WANT TO HAVE AVERAGE VECTOR(AND NOT GLOBAL MEAN), THIS GAVE WORSE PERFORMANCE THAN GLOBAL MEAN\n",
    "'''\n",
    "\n",
    "X_train= [np.array(train_src),np.array(train_mt)]\n",
    "X_train = np.array(X_train)\n",
    "\n",
    "\n",
    "X_test = [np.array(test_src),np.array(test_mt)]\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "\n",
    "#Reshaping if using shape >3\n",
    "nsamples, nx, ny = X_train.shape\n",
    "X_train = X_train.reshape((nx,ny*nsamples))\n",
    "\n",
    "nsamples, nx, ny = X_test.shape\n",
    "X_test = X_test.reshape((nx,ny*nsamples))\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "#Scores\n",
    "train_scores = np.array(train_scores).astype(float)\n",
    "y_train =train_scores\n",
    "\n",
    "test_scores = np.array(test_scores).astype(float)\n",
    "y_test =test_scores\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XSIE7d8HCTpi"
   },
   "source": [
    "### Training the Regressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3eoY14lNCTe3"
   },
   "source": [
    "At this point,  will try SVM and Random Tree Forests and choose the model with the highest Pearson correlation.\n",
    "\n",
    "First we will define our RMSE function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dRcegRvW2F2q"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IerDa2251swL"
   },
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "exHbrWtq14jm"
   },
   "source": [
    "SVM have many parameters such as the kernel and the regularizating constant C. Here we will use C = 1 and compare kernels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QiHCkGUgsJ8r"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "for k in ['linear','poly','rbf','sigmoid']:\n",
    "    clf_t = SVR(kernel=k)\n",
    "    clf_t.fit(X_train_de, y_train_de)\n",
    "    print(k)\n",
    "    predictions = clf_t.predict(X_val_de)\n",
    "    pearson = pearsonr(y_val_de, predictions)\n",
    "    print(f'RMSE: {rmse(predictions,y_val_de)} Pearson {pearson[0]}')\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P1Fr1RLm3-Kc"
   },
   "source": [
    "Here the best kernel seems to be the polynomial one as it gives us the highest pearson correlation at 0.062."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qg9YSBUG1zaL"
   },
   "source": [
    "#### Random Tree Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Au88fVS33K4W"
   },
   "source": [
    "Another powerful regressor is the Random Tree Forest. Here we have to choose the number of trees we want to compute and we will pick n_estimators = 1000. The higher the number the longer it will compute. To fine tune that number you could compute the error per number of trees and select the number for which there is no more significant improvement( the \"elbow\" of the graph)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VOld4zbmsOGL"
   },
   "outputs": [],
   "source": [
    "# Import the model we are using\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 666)\n",
    "\n",
    "rf.fit(X_train_de, y_train_de);\n",
    "\n",
    "\n",
    "predictions = rf.predict(X_val_de)\n",
    "\n",
    "pearson = pearsonr(y_val_de, predictions)\n",
    "print('RMSE:', rmse(predictions,y_val_de))\n",
    "print(f\"Pearson {pearson[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0L73jhAc6ZoM"
   },
   "source": [
    "In this case, it seems like the SVM with a linear kernel performed the best on our validation set so we will save that model for the test set predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G9puD_0zkC2c"
   },
   "source": [
    "### Writing Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oQvvIhPDkUnR"
   },
   "source": [
    "Here is our function to write the scores into a txt file. We can follow the <Method> <ID> <SCORE> template but having only the scores will work too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LN3NtkF4kPxw"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def writeScores(method_name,scores):\n",
    "    fn = \"predictions.txt\"\n",
    "    print(\"\")\n",
    "    with open(fn, 'w') as output_file:\n",
    "        for idx,x in enumerate(scores):\n",
    "            #out =  metrics[idx]+\":\"+str(\"{0:.2f}\".format(x))+\"\\n\"\n",
    "            #print(out)\n",
    "            output_file.write(f\"{x}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FVss_RLBkFei"
   },
   "outputs": [],
   "source": [
    "#EN-DE\n",
    "\n",
    "de_test_src = get_embeddings(\"./test.ende.src\",nlp_en,'en')\n",
    "de_test_mt = get_embeddings(\"./test.ende.mt\",nlp_de,'de')\n",
    "\n",
    "X= [np.array(de_test_src),np.array(de_test_mt)]\n",
    "X_test = np.array(X).transpose()\n",
    "\n",
    "#Predict\n",
    "clf_de = SVR(kernel='rbf')\n",
    "clf_de.fit(X_train_de, y_train_de)\n",
    "\n",
    "predictions_de = clf_de.predict(X_val_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XWnNUR0Gku_9"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "from zipfile import ZipFile\n",
    "\n",
    "\n",
    "writeScores(\"SVR\",predictions_de)\n",
    "\n",
    "with ZipFile(\"en-de_svr.zip\",\"w\") as newzip:\n",
    "\tnewzip.write(\"predictions.txt\")\n",
    " \n",
    "files.download('en-de_svr.zip') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wyaM_P0bynB-"
   },
   "source": [
    "### Results\n",
    "\n",
    "Once submitted to codalab, the pearson correlation is 0.0052."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3obhUYW5ptUS"
   },
   "source": [
    "##B. English-Chinese\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OE9wypehaLrZ"
   },
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_5y34iNipyr3"
   },
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "\n",
    "if not exists('enzh_data.zip'):\n",
    "    !wget -O enzh_data.zip https://competitions.codalab.org/my/datasets/download/03e23bd7-8084-4542-997b-6a1ca6dd8a5f\n",
    "    !unzip enzh_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RlXMiqJXq8fy"
   },
   "outputs": [],
   "source": [
    "#English-Chinese\n",
    "#Checking Data\n",
    "print(\"---EN-ZH---\")\n",
    "print()\n",
    "\n",
    "with open(\"./train.enzh.src\", \"r\") as enzh_src:\n",
    "  print(\"Source: \",enzh_src.readline())\n",
    "with open(\"./train.enzh.mt\", \"r\") as enzh_mt:\n",
    "  print(\"Translation: \",enzh_mt.readline())\n",
    "with open(\"./train.enzh.scores\", \"r\") as enzh_scores:\n",
    "  print(\"Score: \",enzh_scores.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GlOd_5a6aTVP"
   },
   "source": [
    "### Computing Sentence Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZgqjMa_wu0xz"
   },
   "source": [
    "For this task, we will compute the embeddings for words in a sentence in one language and compute the global mean for that sentence, and do the same for the other language. However, we will have to find and download pre-traind embeddings for Chinese as Spacy nor GloVe handle it. The embeddings we will be using for Chinese are of dimension 100, therefore we need to adapt the embeddings for english from the dim 300 to 100. Glove does have English embeddings of dim 100 but Spacy does not have that model. So, we will tokenize the sentences using Spacy tokenizer and use GloVe directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QsKYMxCSolrx"
   },
   "source": [
    "#### Pre-processing English with GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xd24p41jkv7N"
   },
   "source": [
    "With GloVe's function *stoi()* (string to int) we can get the index corresponding to a given word and with the function *itos()* we get the word given its index. To obtain the vector of a word we first get the integer associated with it and then index it into the word embedding tensor with that index. Note that glove takes words in a lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8lc4rdJnrE_Q"
   },
   "outputs": [],
   "source": [
    "# DON'T RUN IF YOU ALREADY RAN IT IN THE ENGLISH-GERMAN SECTION\n",
    "# Downloading spacy models for english\n",
    "\n",
    "!spacy download en_core_web_md\n",
    "!spacy link en_core_web_md en300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fx3Ja9zWFDj2"
   },
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import spacy\n",
    "\n",
    "#Embeddings\n",
    "glove = torchtext.vocab.GloVe(name='6B', dim=100)\n",
    "\n",
    "#tokenizer model\n",
    "nlp_en =spacy.load('en300')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lxjmj7vUv08E"
   },
   "source": [
    "We can now write our functions that will return the average embeddings for a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2BUi2QiCIi9y"
   },
   "outputs": [],
   "source": [
    "#ENGLISH EMBEDDINGS methods from the section GERMAN-ENGLISH\n",
    "# The difference from previous section is that we will use Glove embeddings directly because we are using a smaller model that spacy doesn't have\n",
    "# We add a method to compute the word embedding and a method to compute the sentence embedding by averaging the word vectors\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from nltk import download\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#downloading stopwords from the nltk package\n",
    "download('stopwords') #stopwords dictionary, run once\n",
    "stop_words_en = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def preprocess(sentence,nlp):\n",
    "    text = sentence.lower()\n",
    "    doc = [token.lemma_ for token in  nlp.tokenizer(text)]\n",
    "    doc = [word for word in doc if word not in stop_words_en]\n",
    "    doc = [word for word in doc if word.isalpha()] #restricts string to alphabetic characters only\n",
    "    return doc\n",
    "\n",
    "def get_word_vector(embeddings, word):\n",
    "    try:\n",
    "      vec = embeddings.vectors[embeddings.stoi[word]]\n",
    "      return vec\n",
    "    except KeyError:\n",
    "      #print(f\"Word {word} does not exist\")\n",
    "      pass\n",
    "\n",
    "def get_sentence_vector(embeddings,line):\n",
    "  vectors = []\n",
    "  for w in line:\n",
    "    emb = get_word_vector(embeddings,w)\n",
    "    #do not add if the word is out of vocabulary\n",
    "    if emb is not None:\n",
    "      vectors.append(emb)\n",
    "   \n",
    "  return torch.mean(torch.stack(vectors))\n",
    "\n",
    "\n",
    "def get_embeddings(f,embeddings,lang):\n",
    "  file = open(f) \n",
    "  lines = file.readlines() \n",
    "  sentences_vectors =[]\n",
    "\n",
    "  for l in lines:\n",
    "    sentence= preprocess(l,lang)\n",
    "    try:\n",
    "      vec = get_sentence_vector(embeddings,sentence)\n",
    "      sentences_vectors.append(vec)\n",
    "    except:\n",
    "      sentences_vectors.append(0)\n",
    "\n",
    "  return sentences_vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f4JnbxSaaasu"
   },
   "source": [
    "#### Loading Chinese Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t3-NpUxd52nP"
   },
   "source": [
    "We now have to download the pre-trained embeddings for Chinese. We will get them from the University of Oslo NLPL repository (http://vectors.nlpl.eu/repository/), which has word2vec vectors of dimension 100.\n",
    "\n",
    " We will also get Chinese stop words from https://github.com/Tony607/Chinese_sentiment_analysis.\n",
    "\n",
    "For embeddings of dimensions 300 you can find them searching on github repositories. One example is https://github.com/Kyubyong/wordvectors.\n",
    "\n",
    "If you want to work on colab and download other embeddings I would suggest you download the file and upload it on your dropbox and get the link from there.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-jW3S2-rs6BV"
   },
   "outputs": [],
   "source": [
    "\n",
    "!wget -c https://github.com/Tony607/Chinese_sentiment_analysis/blob/master/data/chinese_stop_words.txt\n",
    "\n",
    "!wget -O zh.zip http://vectors.nlpl.eu/repository/20/35.zip\n",
    "\n",
    "!unzip zh.zip \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AQM6Go4rEe9N"
   },
   "source": [
    "We now load the pre-trained word2vec embeddings we downloaded using the gensim package. More info on gensim and how to use it to load models and embeddings here https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uDUbXQ4aMv1K"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "wv_from_bin = KeyedVectors.load_word2vec_format(\"model.bin\", binary=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uhZ3HtrdodcW"
   },
   "source": [
    "#### Pre-processing Chinese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "her8c6oJFWAa"
   },
   "source": [
    "For pre-processing chinese sentence we will use the tokenizer package for chinese called jieba and use the downloaded list of chinese stop words to remove them from our tokens. More info on jieba and its options at https://github.com/fxsjy/jieba. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-LA9N1zgsSQl"
   },
   "outputs": [],
   "source": [
    "\n",
    "import string\n",
    "import jieba\n",
    "import gensim \n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "stop_words = [ line.rstrip() for line in open('./chinese_stop_words.txt',\"r\", encoding=\"utf-8\") ]\n",
    "\n",
    "\n",
    "def get_sentence_vector_zh(line):\n",
    "  vectors = []\n",
    "  for w in line:\n",
    "    try:\n",
    "      emb = wv_from_bin[w]\n",
    "      vectors.append(emb)\n",
    "    except:\n",
    "      pass #Do not add if the word is out of vocabulary\n",
    "  if vectors:\n",
    "    vectors = np.array(vectors)\n",
    "    return np.mean(vectors)  \n",
    "  else:\n",
    "    return 0\n",
    "\n",
    "\n",
    "def processing_zh(sentence):\n",
    "  seg_list = jieba.lcut(sentence,cut_all=True)\n",
    "  doc = [word for word in seg_list if word not in stop_words]\n",
    "  docs = [e for e in doc if e.isalnum()]\n",
    "  return docs\n",
    "\n",
    "\n",
    "def get_sentence_embeddings_zh(f):\n",
    "  file = open(f) \n",
    "  lines = file.readlines() \n",
    "  sentences_vectors =[]\n",
    "  for l in lines:\n",
    "    sent  = processing_zh(l)\n",
    "    vec = get_sentence_vector_zh(sent)\n",
    "\n",
    "    if vec is not None:\n",
    "      sentences_vectors.append(vec)\n",
    "    else:\n",
    "      print(l)\n",
    "  return sentences_vectors\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6zVjor64tR8D"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "\n",
    "\n",
    "zh_train_mt = get_sentence_embeddings_zh(\"./train.enzh.mt\")\n",
    "zh_train_src = get_embeddings(\"./train.enzh.src\",glove,nlp_en)\n",
    "f_train_scores = open(\"./train.enzh.scores\",'r')\n",
    "zh_train_scores = f_train_scores.readlines()\n",
    "\n",
    "\n",
    "zh_val_src = get_embeddings(\"./dev.enzh.src\",glove,nlp_en)\n",
    "zh_val_mt = get_sentence_embeddings_zh(\"./dev.enzh.mt\")\n",
    "f_val_scores = open(\"./dev.enzh.scores\",'r')\n",
    "zh_val_scores = f_val_scores.readlines()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N-pXbaJzExaE"
   },
   "outputs": [],
   "source": [
    "print(f\"Training mt: {len(zh_train_mt)} Training src: {len(zh_train_src)}\")\n",
    "print()\n",
    "print(f\"Validation mt: {len(zh_val_mt)} Validation src: {len(zh_val_src)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ljBHJpa9ATNf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "X_train= [np.array(zh_train_src),np.array(zh_train_mt)]\n",
    "X_train_zh = np.array(X_train).transpose()\n",
    "\n",
    "X_val = [np.array(zh_val_src),np.array(zh_val_mt)]\n",
    "X_val_zh = np.array(X_val).transpose()\n",
    "\n",
    "#Scores\n",
    "train_scores = np.array(zh_train_scores).astype(float)\n",
    "y_train_zh =train_scores\n",
    "\n",
    "val_scores = np.array(zh_val_scores).astype(float)\n",
    "y_val_zh =val_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "luNEyefram7Z"
   },
   "source": [
    "### Training the Regressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hvCS6pa-yIIB"
   },
   "source": [
    "At this point,  will try SVM and Random Tree Forests and choose the model with the highest Pearson correlation.\n",
    "\n",
    "First we will define our RMSE function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "USalvKtRAvQv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a0wOEUhXgteG"
   },
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9rY29AeVyM1n"
   },
   "source": [
    "SVM have many parameters such as the kernel and the regularizating constant C. Here we will use default C = 1 and compare kernels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bf_aJK0QK8jx"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "for k in ['linear','poly','rbf','sigmoid']:\n",
    "    clf_t = SVR(kernel=k)\n",
    "    clf_t.fit(X_train_zh, y_train_zh)\n",
    "    print(k)\n",
    "    predictions = clf_t.predict(X_val_zh)\n",
    "    pearson = pearsonr(y_val_zh, predictions)\n",
    "    print(f'RMSE: {rmse(predictions,y_val_zh)} Pearson {pearson[0]}')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yaOP4zpOgkFP"
   },
   "source": [
    "In this case, the radial basis function kernel performed the best with a Pearson correlation of 0.1147. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wtg69eGbgmHI"
   },
   "source": [
    "#### Random Tree Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OD22DmWvyPs-"
   },
   "source": [
    "Another powerful regressor is the Random Tree Forest. Here we have to choose the number of trees we want to compute and we will pick n_estimators = 1000. The higher the number the longer it will compute. To fine tune that number you could compute the error per number of trees and select the number for which there is no more significant improvement( the \"elbow\" of the graph)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6wEoExkggqHG"
   },
   "outputs": [],
   "source": [
    "# Import the model we are using\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 666)\n",
    "rf.fit(X_train_zh, y_train_zh);\n",
    "predictions = rf.predict(X_val_zh)\n",
    "\n",
    "pearson = pearsonr(y_val_zh, predictions)\n",
    "print('RMSE:', rmse(predictions,y_val_zh))\n",
    "print(f\"Pearson {pearson[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QWiQ2X6Lj3iG"
   },
   "source": [
    "Finally, we see that SVM with RBF kernel is the best model here. We will now use it to predict on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cuIsX8LNiOJm"
   },
   "source": [
    "### Writing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5SQlcfiCITuC"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def writeScores(method_name,scores):\n",
    "    fn = \"predictions.txt\"\n",
    "    print(\"\")\n",
    "    with open(fn, 'w') as output_file:\n",
    "        for idx,x in enumerate(scores):\n",
    "            #out =  metrics[idx]+\":\"+str(\"{0:.2f}\".format(x))+\"\\n\"\n",
    "            #print(out)\n",
    "            output_file.write(f\"{x}\\n\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VC3ALWVEXYVi"
   },
   "outputs": [],
   "source": [
    "#EN_ZH\n",
    "\n",
    "zh_test_mt = get_sentence_embeddings_zh(\"./test.enzh.mt\")\n",
    "zh_test_src = get_embeddings(\"./test.enzh.src\",glove,nlp_en)\n",
    "\n",
    "X= [np.array(zh_test_mt),np.array(zh_test_src)]\n",
    "X_test_zh = np.array(X).transpose()\n",
    "\n",
    "#Predict\n",
    "clf_zh = SVR(kernel='rbf')\n",
    "clf_zh.fit(X_train_zh, y_train_zh)\n",
    "\n",
    "predictions_zh = clf_zh.predict(X_test_zh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r-nDAsi3Xt-4"
   },
   "outputs": [],
   "source": [
    "#EN_ZH\n",
    "\n",
    "from google.colab import files\n",
    "from zipfile import ZipFile\n",
    "\n",
    "\n",
    "writeScores(\"SVR\",predictions_zh)\n",
    "\n",
    "with ZipFile(\"en-zh_svr.zip\",\"w\") as newzip:\n",
    "\tnewzip.write(\"predictions.txt\")\n",
    " \n",
    "files.download('en-zh_svr.zip') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QlGblmKPyUFr"
   },
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qWUSJ7kAyXxC"
   },
   "source": [
    "Once submitted to codalab, the pearson correlation is 0.0795"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Coursework_baseline.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
